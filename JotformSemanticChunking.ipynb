{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyPDF2 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import openai\n",
    "import PyPDF2  # PDF okuma işlemleri için\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, embeddings, window_size=6, threshold=0.7):\n",
    "        self.embeddings = embeddings\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def split_text_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        Metni cümlelere böler. Regex deseni, \n",
    "          - Bir alfanümerik karakterden sonra gelen nokta ve \n",
    "          - Noktanın sonrasında (boşluklar dahil) büyük harf gelme durumu veya\n",
    "          - Yeni satır karakterini baz alır.\n",
    "        Sonrasında cümleler temizlenir ve 1000 karakteri aşmayan parçalara birleştirilir.\n",
    "        \"\"\"\n",
    "        pattern = r'(?<=[a-zA-Z0-9])\\.(?=\\s*[A-Z])|(?<=\\n)'\n",
    "        sentences = re.split(pattern, text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        merged_sentences = []\n",
    "        buffer = \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(buffer) + len(sentence) < 500:\n",
    "                buffer = f\"{buffer} {sentence}\" if buffer else sentence\n",
    "            else:\n",
    "                if buffer:\n",
    "                    merged_sentences.append(buffer)\n",
    "                buffer = sentence\n",
    "        if buffer:\n",
    "            merged_sentences.append(buffer)\n",
    "        return merged_sentences\n",
    "\n",
    "    def split_sentence_by_word_boundary(self, sentence: str, max_length: int) -> list:\n",
    "        \"\"\"\n",
    "        Bir cümleyi kelime sınırlarına göre böler.\n",
    "        Eğer mevcut parçaya yeni bir kelime eklenince max_length'i aşarsa,\n",
    "        o parçayı listeye ekler ve yeni parçaya kelime eklemeye başlar.\n",
    "        \"\"\"\n",
    "        words = sentence.split()\n",
    "        parts = []\n",
    "        current_part = \"\"\n",
    "        for word in words:\n",
    "            if len(current_part + \" \" + word) > max_length:\n",
    "                parts.append(current_part.strip())\n",
    "                current_part = word\n",
    "            else:\n",
    "                current_part += \" \" + word\n",
    "        if current_part.strip():\n",
    "            parts.append(current_part.strip())\n",
    "        return parts\n",
    "\n",
    "    def preprocess_sentences(self, sentences: list) -> list:\n",
    "        \"\"\"\n",
    "        Cümleleri ön işlemden geçirir:\n",
    "          - Uzunluğu 3 karakterden kısa olanları atlar.\n",
    "          - 10000 karakterden uzun cümleleri kelime bazında bölerek parçalar.\n",
    "          - Diğer cümleleri olduğu gibi ekler.\n",
    "        \"\"\"\n",
    "        processed_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if len(sentence) < 3:\n",
    "                continue\n",
    "            if len(sentence) > 10000:\n",
    "                splits = self.split_sentence_by_word_boundary(sentence, 10000)\n",
    "                processed_sentences.extend(splits)\n",
    "            else:\n",
    "                processed_sentences.append(sentence)\n",
    "        return processed_sentences\n",
    "\n",
    "    def create_embeddings(self, texts: list) -> list:\n",
    "        \"\"\"\n",
    "        Embeddings sağlayıcısını kullanarak verilen metinler için embedding'leri döndürür.\n",
    "        \"\"\"\n",
    "        return self.embeddings.embed_documents(texts)\n",
    "\n",
    "    def get_embeddings_batch(self, sentences: list, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Cümleleri 2048'lik partiler halinde işleyerek embedding'leri toplar.\n",
    "        \"\"\"\n",
    "        batch_size = 2048\n",
    "        embeddings_result = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i+batch_size]\n",
    "            batch_result = self.create_embeddings(batch)\n",
    "            if not batch_result:\n",
    "                print(f\"Error: Embedding API returned no data for batch {i // batch_size} and agent {agent_id}\")\n",
    "            embeddings_result.extend(batch_result)\n",
    "        return embeddings_result\n",
    "\n",
    "    def calculate_divergence(self, embedding1: list, embedding2: list) -> float:\n",
    "        \"\"\"\n",
    "        İki embedding arasındaki farkı (divergence) hesaplar.\n",
    "        Hesaplama, kosinüs benzerliği esasına göre yapılır; sonuç 1 - cosine_similarity olarak döner.\n",
    "        \"\"\"\n",
    "        if len(embedding1) != len(embedding2):\n",
    "            return 0.0\n",
    "        dot_product = 0.0\n",
    "        magnitude1 = 0.0\n",
    "        magnitude2 = 0.0\n",
    "        for v1, v2 in zip(embedding1, embedding2):\n",
    "            dot_product += v1 * v2\n",
    "            magnitude1 += v1 ** 2\n",
    "            magnitude2 += v2 ** 2\n",
    "        magnitude1 = math.sqrt(magnitude1)\n",
    "        magnitude2 = math.sqrt(magnitude2)\n",
    "        if magnitude1 == 0 or magnitude2 == 0:\n",
    "            return 0.0\n",
    "        cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "        return 1 - cosine_similarity\n",
    "\n",
    "    def calculate_average_embedding(self, embeddings: list) -> list:\n",
    "        \"\"\"\n",
    "        Bir grup embedding'in ortalamasını hesaplar.\n",
    "        \"\"\"\n",
    "        if not embeddings:\n",
    "            return []\n",
    "        num_embeddings = len(embeddings)\n",
    "        vector_length = len(embeddings[0])\n",
    "        sum_vector = [0.0] * vector_length\n",
    "        for embedding in embeddings:\n",
    "            for i, value in enumerate(embedding):\n",
    "                sum_vector[i] += value\n",
    "        average_vector = [s / num_embeddings for s in sum_vector]\n",
    "        return average_vector\n",
    "\n",
    "    def sliding_window_divergence(self, sentences: list, window_size: int, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Kayan pencere yöntemiyle, belirli pencere boyutundaki cümle gruplarının divergence değerlerini hesaplar.\n",
    "        \"\"\"\n",
    "        divergences = []\n",
    "        embeddings = self.get_embeddings_batch(sentences, agent_id)\n",
    "        half_window = window_size // 2\n",
    "        for i in range(0, len(sentences) - window_size + 1):\n",
    "            window1_embeddings = embeddings[i:i+half_window]\n",
    "            window2_embeddings = embeddings[i+half_window:i+half_window+half_window]\n",
    "            avg_embedding1 = self.calculate_average_embedding(window1_embeddings)\n",
    "            avg_embedding2 = self.calculate_average_embedding(window2_embeddings)\n",
    "            divergence = self.calculate_divergence(avg_embedding1, avg_embedding2)\n",
    "            divergences.append(divergence)\n",
    "        return divergences\n",
    "\n",
    "    def detect_peaks(self, divergences: list, threshold: float) -> list:\n",
    "        \"\"\"\n",
    "        Hesaplanan divergence değerleri arasında, belirlenen eşik değerinin üzerinde ve komşularına göre zirve olan noktaları tespit eder.\n",
    "        \"\"\"\n",
    "        peaks = []\n",
    "        if not divergences:\n",
    "            return peaks\n",
    "        max_divergence = max(divergences)\n",
    "        threshold_value = threshold * max_divergence\n",
    "        for i, value in enumerate(divergences):\n",
    "            prev_val = divergences[i-1] if i - 1 >= 0 else 0\n",
    "            next_val = divergences[i+1] if i + 1 < len(divergences) else 0\n",
    "            if value > threshold_value and value > prev_val and value > next_val:\n",
    "                peaks.append(i)\n",
    "        return peaks\n",
    "\n",
    "    def semantic_chunking(self, text: str, window_size: int, threshold: float, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Metni anlamsal parçalara böler:\n",
    "          1. Metin cümlelere ayrılır.\n",
    "          2. Kayan pencere yöntemiyle divergence değerleri hesaplanır.\n",
    "          3. Tespit edilen peak noktaları, metni bölecek yerler olarak kullanılır.\n",
    "          4. Cümle grupları (chunk) oluşturulur.\n",
    "        \"\"\"\n",
    "        sentences = self.split_text_into_sentences(text)\n",
    "\n",
    "        sentences = self.preprocess_sentences(sentences)\n",
    "        divergences = self.sliding_window_divergence(sentences, window_size, agent_id)\n",
    "        split_points = self.detect_peaks(divergences, threshold)\n",
    "        chunks = []\n",
    "        last_split = 0\n",
    "        for point in split_points:\n",
    "            chunk = \". \".join(sentences[last_split:point])\n",
    "            chunks.append(chunk)\n",
    "            last_split = point\n",
    "        \n",
    "        chunk = \". \".join(sentences[last_split:])\n",
    "        chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def create_documents(self, docs: list) -> list:\n",
    "        \"\"\"\n",
    "        Verilen metin (doküman) listesini semantic chunking algoritması ile parçalara ayırır \n",
    "        ve her parçayı basit bir doküman olarak döndürür.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for doc in docs:\n",
    "            chunks = self.semantic_chunking(doc, self.window_size, self.threshold, agent_id=\"live\")\n",
    "            for chunk in chunks:\n",
    "                all_chunks.append({\"page_content\": chunk})\n",
    "        return all_chunks\n",
    "\n",
    "def integrate_pipeline(texts: list, material_ids: list, chunker: SemanticChunker,\n",
    "                       window_size: int = 6, threshold: float = 0.7, long_text_limit: int = 10000) -> tuple:\n",
    "    \"\"\"\n",
    "    Verilen metinleri ve ilgili material ID'lerini kontrol eder.\n",
    "    Eğer metin uzunluğu long_text_limit (default: 10000) karakterden fazlaysa,\n",
    "    semantic_chunking ile anlamsal parçalara ayrılır ve her parçaya orijinal material ID atanır.\n",
    "    Aksi halde, metin doğrudan eklenir.\n",
    "    Son olarak, tüm işlenmiş metinler preprocess_sentences ile tekrar işlenir.\n",
    "    \n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    processed_material_ids = []\n",
    "    \n",
    "    for text, mat_id in zip(texts, material_ids):\n",
    "        if len(text) >= long_text_limit:\n",
    "           \n",
    "            chunked_data = chunker.semantic_chunking(text, window_size, threshold, agent_id=\"live\")\n",
    "            processed_data.extend(chunked_data)\n",
    "           \n",
    "            processed_material_ids.extend([mat_id] * len(chunked_data))\n",
    "        else:\n",
    "            processed_data.append(text)\n",
    "            processed_material_ids.append(mat_id)\n",
    "    \n",
    "    processed_data = chunker.preprocess_sentences(processed_data)\n",
    "    \n",
    "    return processed_data, processed_material_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jotform-pdf-LLM_data_collection koleksiyonu silindi, yeniden oluşturuluyor...\n",
      "jotform-pdf-LLM_data_collection koleksiyonu başarıyla oluşturuldu.\n",
      "16 adet semantic chunk Qdrant koleksiyonuna eklendi.\n"
     ]
    }
   ],
   "source": [
    "# RAG ve Qdrant İşlemleri\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "OPENAI_API_KEY = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "collection_name = \"jotform-pdf-LLM_data_collection\"  # Koleksiyon ismi\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=embedding_model_name)\n",
    "\n",
    "chunker = SemanticChunker(embeddings, window_size=6, threshold=0.7)\n",
    "\n",
    "pdf_path = \"Foundations of LLM.pdf\"\n",
    "documents = []\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text.append(page_text)\n",
    " \n",
    "    documents = [\"\\n\".join(full_text)]\n",
    "\n",
    "material_ids = list(range(len(documents)))\n",
    "processed_texts, processed_material_ids = integrate_pipeline(documents, material_ids, chunker)\n",
    "\n",
    "all_chunks = [{\"page_content\": text, \"material_id\": mat_id} for text, mat_id in zip(processed_texts, processed_material_ids)]\n",
    "\n",
    "vector_size = 1536\n",
    "\n",
    "existing_collections = [c.name for c in qdrant_client.get_collections().collections]\n",
    "if collection_name in existing_collections:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"{collection_name} koleksiyonu silindi, yeniden oluşturuluyor...\")\n",
    "\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    ")\n",
    "print(f\"{collection_name} koleksiyonu başarıyla oluşturuldu.\")\n",
    "\n",
    "points = []\n",
    "for chunk in all_chunks:\n",
    "    chunk_text = chunk[\"page_content\"].strip()\n",
    "    if not chunk_text:\n",
    "        continue\n",
    "    vector = embeddings.embed_query(chunk_text)\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\"text\": chunk_text, \"material_id\": chunk[\"material_id\"]}\n",
    "    }\n",
    "    points.append(point)\n",
    "\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "print(f\"{len(points)} adet semantic chunk Qdrant koleksiyonuna eklendi.\")\n",
    "\n",
    "\n",
    "def test_rag_direct_qdrant(qdrant_client, collection_name, query):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [result.payload[\"text\"] for result in search_results]\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        print(\"Qdrant'ta eşleşen belge bulunamadı.\")\n",
    "        return\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    print(\"\\nQdrant'tan Kullanılan Kaynaklar:\")\n",
    "    for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "        print(f\"{idx}. (Uzunluk: {len(doc)}) {doc[:100]}...\")\n",
    "\n",
    "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer: \"\n",
    "\n",
    "   \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that provides precise answers based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nInference zamanı: {round(end_time - start_time, 3)} saniye\")\n",
    "    print(f\"\\nModelin cevabı:\\n{response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 10000) . . . . 172 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3...\n",
      "2. (Uzunluk: 9993) entire model is traine d from scratch A well-known example of this is training sequence models by su...\n",
      "3. (Uzunluk: 9996) architectures Moreover, we have presented and com pared a variety of pre-training tasks for these ar...\n",
      "4. (Uzunluk: 9995) source and target sides, respect ively5 Likewise, we can express other tasks in the same way For exa...\n",
      "5. (Uzunluk: 9994) corresponding output The notation of this equation seems a bit complicated, but the training/tuning ...\n",
      "\n",
      "Inference zamanı: 2.792 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Pre-training refers to the process of optimizing a neural network on a large dataset before it is further trained or fine-tuned for specific tasks. This approach leverages self-supervised, unsupervised, or supervised learning methods to create a model that can generalize across various tasks. The main goal of pre-training is to reduce the reliance on task-specific labeled data, allowing the model to learn useful representations from vast amounts of unlabeled data. Pre-trained models can then be adapted to specific tasks through fine-tuning or prompting, significantly improving performance in natural language processing (NLP) and other domains.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Which types of models are widely used in NLP pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 9996) architectures Moreover, we have presented and com pared a variety of pre-training tasks for these ar...\n",
      "2. (Uzunluk: 9993) entire model is traine d from scratch A well-known example of this is training sequence models by su...\n",
      "3. (Uzunluk: 10000) . . . . 172 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3...\n",
      "4. (Uzunluk: 9999) important roles in t he recent rise of large language models We will discuss these issues more deepl...\n",
      "5. (Uzunluk: 9998) arXiv:2501.09223v1 [cs CL] 16 Jan 2025Foundations of Large Language Models Tong Xiao and Jingbo Zhu ...\n",
      "\n",
      "Inference zamanı: 4.448 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The two major types of models widely used in NLP pre-training are:\n",
      "\n",
      "1. **Sequence Encoding Models**: These models represent a sequence of words or tokens as either a real-valued vector or a sequence of vectors, which is typically used as input to another model, such as a sentence classification system.\n",
      "\n",
      "2. **Sequence Generation Models**: These models generate a sequence of tokens based on a given context. They are often employed independently to address language generation problems, such as question answering and machine translation.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Which types of models are widely used in NLP pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How do we implement permuted language modelling?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 10000) right-to-left), permuted language modeling allows for predictions in any order The approach i s stra...\n",
      "2. (Uzunluk: 9999) important roles in t he recent rise of large language models We will discuss these issues more deepl...\n",
      "3. (Uzunluk: 9998) seen as sub- sequences from the same sequence By adopting such notation , we see that the form of th...\n",
      "4. (Uzunluk: 9996) i−1)is the value of the i-th entry of. Pr(·|x0,...,x i−1). When applying a trained language model, a...\n",
      "5. (Uzunluk: 9996) architectures Moreover, we have presented and com pared a variety of pre-training tasks for these ar...\n",
      "\n",
      "Inference zamanı: 3.711 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Permuted language modeling can be implemented by determining a specific order for token predictions that differs from the standard left-to-right sequence while keeping the original order of tokens in the text unchanged. Here’s a step-by-step outline of the implementation:\n",
      "\n",
      "1. **Define the Token Sequence**: Start with a sequence of tokens, for example, \\(x_0, x_1, x_2, x_3, x_4\\).\n",
      "\n",
      "2. **Determine Prediction Order**: Choose a new order for predicting the tokens, such as \\(x_0 \\rightarrow x_4 \\rightarrow x_2 \\rightarrow x_1 \\rightarrow x_3\\).\n",
      "\n",
      "3. **Model the Probability**: Express the probability of the sequence based on the new prediction order:\n",
      "   \\[\n",
      "   Pr(x) = Pr(x_0) \\cdot Pr(x_4 | e_0) \\cdot Pr(x_2 | e_0, e_4) \\cdot Pr(x_1 | e_0, e_4, e_2) \\cdot Pr(x_3 | e_0, e_4, e_2, e_1)\n",
      "   \\]\n",
      "   Here, \\(e_i\\) represents the embedding of token \\(x_i\\).\n",
      "\n",
      "4. **Use Self-Attention with Masks**: In a Transformer architecture, implement self-attention with appropriate masks to block attention to certain tokens based on the defined prediction order. For example,\n"
     ]
    }
   ],
   "source": [
    "query3 = \"How do we implement permuted language modelling?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the large-scale pre-training of the document?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 9996) architectures Moreover, we have presented and com pared a variety of pre-training tasks for these ar...\n",
      "2. (Uzunluk: 9998) arXiv:2501.09223v1 [cs CL] 16 Jan 2025Foundations of Large Language Models Tong Xiao and Jingbo Zhu ...\n",
      "3. (Uzunluk: 10000) . . . . 172 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3...\n",
      "4. (Uzunluk: 9993) entire model is traine d from scratch A well-known example of this is training sequence models by su...\n",
      "5. (Uzunluk: 9999) important roles in t he recent rise of large language models We will discuss these issues more deepl...\n",
      "\n",
      "Inference zamanı: 2.457 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Large-scale pre-training refers to the process of training AI models, particularly in natural language processing (NLP), on vast amounts of unlabeled data using self-supervised learning techniques. This approach allows models to learn general knowledge and linguistic structures by predicting masked words or tokens in a large corpus of text. For example, models like BERT utilize masked language modeling as a pre-training task, enabling them to serve as foundation models that can be easily adapted to various downstream tasks, such as text classification or question answering. This paradigm shift has significantly advanced NLP capabilities, leading to the development of powerful systems that can understand and generate human-like language, while reducing the need for extensive labeled datasets for specific tasks.\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is the large-scale pre-training of the document?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
