{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import openai\n",
    "import PyPDF2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, embeddings, window_size=6, threshold=0.7):\n",
    "        self.embeddings = embeddings\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def split_text_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        Metni cümlelere böler. Regex deseni, \n",
    "          - Bir alfanümerik karakterden sonra gelen nokta ve \n",
    "          - Noktanın sonrasında (boşluklar dahil) büyük harf gelme durumu veya\n",
    "          - Yeni satır karakterini baz alır.\n",
    "        Sonrasında cümleler temizlenir ve 1000 karakteri aşmayan parçalara birleştirilir.\n",
    "        \"\"\"\n",
    "        pattern = r'(?<=[a-zA-Z0-9])\\.(?=\\s*[A-Z])|(?<=\\n)'\n",
    "        sentences = re.split(pattern, text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        merged_sentences = []\n",
    "        buffer = \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(buffer) + len(sentence) < 1000:\n",
    "                buffer = f\"{buffer} {sentence}\" if buffer else sentence\n",
    "            else:\n",
    "                if buffer:\n",
    "                    merged_sentences.append(buffer)\n",
    "                buffer = sentence\n",
    "        if buffer:\n",
    "            merged_sentences.append(buffer)\n",
    "        return merged_sentences\n",
    "\n",
    "    def split_sentence_by_word_boundary(self, sentence: str, max_length: int) -> list:\n",
    "        \"\"\"\n",
    "        Bir cümleyi kelime sınırlarına göre böler.\n",
    "        Eğer mevcut parçaya yeni bir kelime eklenince max_length'i aşarsa,\n",
    "        o parçayı listeye ekler ve yeni parçaya kelime eklemeye başlar.\n",
    "        \"\"\"\n",
    "        words = sentence.split()\n",
    "        parts = []\n",
    "        current_part = \"\"\n",
    "        for word in words:\n",
    "            if len(current_part + \" \" + word) > max_length:\n",
    "                parts.append(current_part.strip())\n",
    "                current_part = word\n",
    "            else:\n",
    "                current_part += \" \" + word\n",
    "        if current_part.strip():\n",
    "            parts.append(current_part.strip())\n",
    "        return parts\n",
    "\n",
    "    def preprocess_sentences(self, sentences: list) -> list:\n",
    "        \"\"\"\n",
    "        Cümleleri ön işlemden geçirir:\n",
    "          - Uzunluğu 3 karakterden kısa olanları atlar.\n",
    "          - 10000 karakterden uzun cümleleri kelime bazında bölerek parçalar.\n",
    "          - Diğer cümleleri olduğu gibi ekler.\n",
    "        \"\"\"\n",
    "        processed_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if len(sentence) < 3:\n",
    "                continue\n",
    "            if len(sentence) > 10000:\n",
    "                splits = self.split_sentence_by_word_boundary(sentence, 10000)\n",
    "                processed_sentences.extend(splits)\n",
    "            else:\n",
    "                processed_sentences.append(sentence)\n",
    "        return processed_sentences\n",
    "\n",
    "    def create_embeddings(self, texts: list) -> list:\n",
    "        \"\"\"\n",
    "        Embeddings sağlayıcısını kullanarak verilen metinler için embedding'leri döndürür.\n",
    "        \"\"\"\n",
    "        return self.embeddings.embed_documents(texts)\n",
    "\n",
    "    def get_embeddings_batch(self, sentences: list, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Cümleleri 2048'lik partiler halinde işleyerek embedding'leri toplar.\n",
    "        \"\"\"\n",
    "        batch_size = 2048\n",
    "        embeddings_result = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i+batch_size]\n",
    "            batch_result = self.create_embeddings(batch)\n",
    "            if not batch_result:\n",
    "                print(f\"Error: Embedding API returned no data for batch {i // batch_size} and agent {agent_id}\")\n",
    "            embeddings_result.extend(batch_result)\n",
    "        return embeddings_result\n",
    "\n",
    "    def calculate_divergence(self, embedding1: list, embedding2: list) -> float:\n",
    "        \"\"\"\n",
    "        İki embedding arasındaki farkı (divergence) hesaplar.\n",
    "        Hesaplama, kosinüs benzerliği esasına göre yapılır; sonuç 1 - cosine_similarity olarak döner.\n",
    "        \"\"\"\n",
    "        if len(embedding1) != len(embedding2):\n",
    "            return 0.0\n",
    "        dot_product = 0.0\n",
    "        magnitude1 = 0.0\n",
    "        magnitude2 = 0.0\n",
    "        for v1, v2 in zip(embedding1, embedding2):\n",
    "            dot_product += v1 * v2\n",
    "            magnitude1 += v1 ** 2\n",
    "            magnitude2 += v2 ** 2\n",
    "        magnitude1 = math.sqrt(magnitude1)\n",
    "        magnitude2 = math.sqrt(magnitude2)\n",
    "        if magnitude1 == 0 or magnitude2 == 0:\n",
    "            return 0.0\n",
    "        cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "        return 1 - cosine_similarity\n",
    "\n",
    "    def calculate_average_embedding(self, embeddings: list) -> list:\n",
    "        \"\"\"\n",
    "        Bir grup embedding'in ortalamasını hesaplar.\n",
    "        \"\"\"\n",
    "        if not embeddings:\n",
    "            return []\n",
    "        num_embeddings = len(embeddings)\n",
    "        vector_length = len(embeddings[0])\n",
    "        sum_vector = [0.0] * vector_length\n",
    "        for embedding in embeddings:\n",
    "            for i, value in enumerate(embedding):\n",
    "                sum_vector[i] += value\n",
    "        average_vector = [s / num_embeddings for s in sum_vector]\n",
    "        return average_vector\n",
    "\n",
    "    def sliding_window_divergence(self, sentences: list, window_size: int, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Kayan pencere yöntemiyle, belirli pencere boyutundaki cümle gruplarının divergence değerlerini hesaplar.\n",
    "        \"\"\"\n",
    "        divergences = []\n",
    "        embeddings = self.get_embeddings_batch(sentences, agent_id)\n",
    "        half_window = window_size // 2\n",
    "        for i in range(0, len(sentences) - window_size + 1):\n",
    "            window1_embeddings = embeddings[i:i+half_window]\n",
    "            window2_embeddings = embeddings[i+half_window:i+half_window+half_window]\n",
    "            avg_embedding1 = self.calculate_average_embedding(window1_embeddings)\n",
    "            avg_embedding2 = self.calculate_average_embedding(window2_embeddings)\n",
    "            divergence = self.calculate_divergence(avg_embedding1, avg_embedding2)\n",
    "            divergences.append(divergence)\n",
    "        return divergences\n",
    "\n",
    "    def detect_peaks(self, divergences: list, threshold: float) -> list:\n",
    "        \"\"\"\n",
    "        Hesaplanan divergence değerleri arasında, belirlenen eşik değerinin üzerinde ve komşularına göre zirve olan noktaları tespit eder.\n",
    "        \"\"\"\n",
    "        peaks = []\n",
    "        if not divergences:\n",
    "            return peaks\n",
    "        max_divergence = max(divergences)\n",
    "        threshold_value = threshold * max_divergence\n",
    "        for i, value in enumerate(divergences):\n",
    "            prev_val = divergences[i-1] if i - 1 >= 0 else 0\n",
    "            next_val = divergences[i+1] if i + 1 < len(divergences) else 0\n",
    "            if value > threshold_value and value > prev_val and value > next_val:\n",
    "                peaks.append(i)\n",
    "        return peaks\n",
    "\n",
    "    def semantic_chunking(self, text: str, window_size: int, threshold: float, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Metni anlamsal parçalara böler:\n",
    "          1. Metin cümlelere ayrılır.\n",
    "          2. Kayan pencere yöntemiyle divergence değerleri hesaplanır.\n",
    "          3. Tespit edilen peak noktaları, metni bölecek yerler olarak kullanılır.\n",
    "          4. Cümle grupları (chunk) oluşturulur.\n",
    "        \"\"\"\n",
    "        sentences = self.split_text_into_sentences(text)\n",
    "\n",
    "        sentences = self.preprocess_sentences(sentences)\n",
    "        divergences = self.sliding_window_divergence(sentences, window_size, agent_id)\n",
    "        split_points = self.detect_peaks(divergences, threshold)\n",
    "        chunks = []\n",
    "        last_split = 0\n",
    "        for point in split_points:\n",
    "            chunk = \". \".join(sentences[last_split:point])\n",
    "            chunks.append(chunk)\n",
    "            last_split = point\n",
    "        \n",
    "        chunk = \". \".join(sentences[last_split:])\n",
    "        chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def create_documents(self, docs: list) -> list:\n",
    "        \"\"\"\n",
    "        Verilen metin (doküman) listesini semantic chunking algoritması ile parçalara ayırır \n",
    "        ve her parçayı basit bir doküman olarak döndürür.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for doc in docs:\n",
    "            chunks = self.semantic_chunking(doc, self.window_size, self.threshold, agent_id=\"live\")\n",
    "            for chunk in chunks:\n",
    "                all_chunks.append({\"page_content\": chunk})\n",
    "        return all_chunks\n",
    "\n",
    "def integrate_pipeline(texts: list, material_ids: list, chunker: SemanticChunker,\n",
    "                       window_size: int = 6, threshold: float = 0.7, long_text_limit: int = 10000) -> tuple:\n",
    "    \"\"\"\n",
    "    Verilen metinleri ve ilgili material ID'lerini kontrol eder.\n",
    "    Eğer metin uzunluğu long_text_limit (default: 10000) karakterden fazlaysa,\n",
    "    semantic_chunking ile anlamsal parçalara ayrılır ve her parçaya orijinal material ID atanır.\n",
    "    Aksi halde, metin doğrudan eklenir.\n",
    "    Son olarak, tüm işlenmiş metinler preprocess_sentences ile tekrar işlenir.\n",
    "    \n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    processed_material_ids = []\n",
    "    \n",
    "    for text, mat_id in zip(texts, material_ids):\n",
    "        if len(text) >= long_text_limit:\n",
    "           \n",
    "            chunked_data = chunker.semantic_chunking(text, window_size, threshold, agent_id=\"live\")\n",
    "            processed_data.extend(chunked_data)\n",
    "           \n",
    "            processed_material_ids.extend([mat_id] * len(chunked_data))\n",
    "        else:\n",
    "            processed_data.append(text)\n",
    "            processed_material_ids.append(mat_id)\n",
    "    \n",
    "    processed_data = chunker.preprocess_sentences(processed_data)\n",
    "    \n",
    "    return processed_data, processed_material_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import uuid\n",
    "import time\n",
    "import PyPDF2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "def get_embedding(text, model):\n",
    "    response = openai.Embedding.create(input=[text], model=model)\n",
    "    return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "OPENAI_API_KEY = config[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "collection_name = \"jotform-pdf-LLM_data_collection\"\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "\n",
    "def embed_text(text):\n",
    "    return get_embedding(text, embedding_model_name)\n",
    "\n",
    "class EmbeddingWrapper:\n",
    "    def __init__(self, embedding_func):\n",
    "        self.embedding_func = embedding_func\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        # Apply the embedding function to each text in the list\n",
    "        return [self.embedding_func(text) for text in texts]\n",
    "\n",
    "# Wrap your embed_text function in the EmbeddingWrapper\n",
    "embedding_wrapper = EmbeddingWrapper(embed_text)\n",
    "\n",
    "# Pass the wrapper to the SemanticChunker\n",
    "chunker = SemanticChunker(embedding_wrapper, window_size=6, threshold=0.7)\n",
    "\n",
    "\n",
    "pdf_path = \"Foundations of LLM.pdf\"\n",
    "documents = []\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text.append(page_text)\n",
    "    documents = [\"\\n\".join(full_text)]\n",
    "\n",
    "material_ids = list(range(len(documents)))\n",
    "processed_texts, processed_material_ids = integrate_pipeline(documents, material_ids, chunker)\n",
    "all_chunks = [{\"page_content\": text, \"material_id\": mid} for text, mid in zip(processed_texts, processed_material_ids)]\n",
    "\n",
    "vector_size = 3072\n",
    "\n",
    "existing_collections = [c.name for c in qdrant_client.get_collections().collections]\n",
    "if collection_name in existing_collections:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "qdrant_client.create_collection(collection_name=collection_name, vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE))\n",
    "points = []\n",
    "for chunk in all_chunks:\n",
    "    chunk_text = chunk[\"page_content\"].strip()\n",
    "    if not chunk_text:\n",
    "        continue\n",
    "    vector = embed_text(chunk_text)\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\"text\": chunk_text, \"material_id\": chunk[\"material_id\"]}\n",
    "    }\n",
    "    points.append(point)\n",
    "qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "def test_rag_direct_qdrant(client, coll_name, query):\n",
    "    start_time = time.time()\n",
    "    query_vector = embed_text(query)\n",
    "    search_results = client.search(collection_name=coll_name, query_vector=query_vector, limit=5, with_payload=True)\n",
    "    retrieved_docs = [result.payload[\"text\"] for result in search_results]\n",
    "    if not retrieved_docs:\n",
    "        print(\"Qdrant'ta eşleşen belge bulunamadı.\")\n",
    "        return\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    print(\"Qdrant'tan Kullanılan Kaynaklar:\")\n",
    "    for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "        print(f\"{idx}. (Uzunluk: {len(doc)}) {doc[:100]}...\")\n",
    "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer: \"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that provides precise answers based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nInference zamanı: {round(end_time - start_time, 3)} saniye\")\n",
    "    print(f\"\\nModelin cevabı:\\n{response.choices[0].message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 10000) 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3.2 Training ...\n",
      "2. (Uzunluk: 9999) d from scratch A well-known example of this is training sequence models by successively predict ing ...\n",
      "3. (Uzunluk: 9999) standard We optimize the model by minimizing the loss over the tuning samples The out come is the op...\n",
      "4. (Uzunluk: 9994) After all these events, Tom has 17 marbles We can add these reasoning steps into the prompt, and get...\n",
      "5. (Uzunluk: 9998) large language models We will discuss these issues more deeply in Chapter 3 . However, it is worth n...\n",
      "\n",
      "Inference zamanı: 3.672 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Pre-training is the process of optimizing a neural network on a large dataset before it is further trained or fine-tuned for specific tasks. This approach leverages the idea that a model trained on one task can be adapted to perform another task, reducing the need for extensive labeled data. Pre-training can be conducted using various methods, including unsupervised learning, supervised learning, and self-supervised learning. In the context of natural language processing (NLP), pre-training often involves training models on vast amounts of unlabeled text data to develop general language understanding and generation capabilities, which can then be fine-tuned or adapted for specific downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 9999) standard We optimize the model by minimizing the loss over the tuning samples The out come is the op...\n",
      "2. (Uzunluk: 9999) d from scratch A well-known example of this is training sequence models by successively predict ing ...\n",
      "3. (Uzunluk: 10000) 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3.2 Training ...\n",
      "4. (Uzunluk: 9992) [CLS] It is raining . [SEP] I need an hat. [SEP] This helps the model learn to recover a token from ...\n",
      "5. (Uzunluk: 9997) BERT is used to illus trate how sequence models are pre- trained via masked language modeling and ap...\n",
      "\n",
      "Inference zamanı: 3.958 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The two major types of models widely used in NLP pre-training are:\n",
      "\n",
      "1. **Sequence Encoding Models**: These models represent a sequence of words or tokens as either a real-valued vector or a sequence of vectors, which is typically used as input to another model, such as a sentence classification system.\n",
      "\n",
      "2. **Sequence Generation Models**: These models are designed to generate a sequence of tokens based on a given context, and they are often employed independently to address language generation problems, such as question answering and machine translation.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Which types of models are widely used in NLP pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 9998) large language models We will discuss these issues more deeply in Chapter 3 . However, it is worth n...\n",
      "2. (Uzunluk: 9997) BERT is used to illus trate how sequence models are pre- trained via masked language modeling and ap...\n",
      "3. (Uzunluk: 10000) token given its previous context tokens This token prediction task can be described as ˆxi= arg max ...\n",
      "4. (Uzunluk: 10000) express other tasks in the same way For exam ple [CLS] Answer: when was Albert Einstein born? → /a\\}...\n",
      "5. (Uzunluk: 9997) the input sequence are randomly selecte d and masked. •Token Deletion . This method is similar to to...\n",
      "\n",
      "Inference zamanı: 5.523 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Permuted language modeling involves making sequential predictions of tokens in a shuffled order rather than following the natural sequence of the text. Here’s how to implement it:\n",
      "\n",
      "1. **Input Preparation**: Start with a sequence of tokens, for example, `x = {x0, x1, x2, ..., xm}`.\n",
      "\n",
      "2. **Permutation**: Randomly shuffle the order of the tokens in the sequence to create a permuted sequence, denoted as `x'`. For example, if the original sequence is `The kitten is chasing the ball`, a possible permutation could be `is The ball chasing kitten the`.\n",
      "\n",
      "3. **Prediction Setup**: Define a prediction task where the model predicts the tokens in the permuted sequence based on a context that includes previously predicted tokens. The model should learn to predict each token based on a subset of tokens that are randomly selected from the permuted sequence.\n",
      "\n",
      "4. **Training Objective**: The training objective is to maximize the likelihood of the original tokens given the permuted context. This can be expressed mathematically as:\n",
      "   \\[\n",
      "   \\hat{\\theta} = \\arg \\max_{\\theta} \\sum_{i=1}^{m} \\log P_{\\theta}(x_i | x'_{context})\n",
      "   \\]\n",
      "   where \\(x'_{context}\\) includes the tokens that are available for prediction at each step.\n",
      "\n",
      "5. **Model Architecture**: Use a model architecture that supports self-attention mechanisms, such as a Transformer\n"
     ]
    }
   ],
   "source": [
    "query3 = \"How do we implement permuted language modelling?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 9994) After all these events, Tom has 17 marbles We can add these reasoning steps into the prompt, and get...\n",
      "2. (Uzunluk: 10000) 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3.2 Training ...\n",
      "3. (Uzunluk: 9999) standard We optimize the model by minimizing the loss over the tuning samples The out come is the op...\n",
      "4. (Uzunluk: 9999) d from scratch A well-known example of this is training sequence models by successively predict ing ...\n",
      "5. (Uzunluk: 9992) [CLS] It is raining . [SEP] I need an hat. [SEP] This helps the model learn to recover a token from ...\n",
      "\n",
      "Inference zamanı: 3.918 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The large-scale pre-training of the document refers to the process of training large language models (LLMs) on vast amounts of data to enhance their ability to understand and generate human language. This involves optimizing neural networks using self-supervised learning techniques, where the models learn from unlabeled data by predicting masked words or generating text based on context. The goal is to create foundation models that can be fine-tuned or prompted for various downstream tasks without needing extensive labeled datasets for each specific task. The document discusses the importance of data quality, diversity, and the challenges associated with training large models, including issues like bias and privacy concerns. It highlights the significant advancements in NLP due to large-scale pre-training, enabling models to achieve remarkable performance across a wide range of applications.\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is the large-scale pre-training of the document?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
