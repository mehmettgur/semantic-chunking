{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import openai\n",
    "import PyPDF2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "def get_embedding(text, model):\n",
    "    response = openai.Embedding.create(input=[text], model=model)\n",
    "    return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, embedding_model, window_size=6, threshold=0.7):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def split_text_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        Metni cümlelere böler. Regex deseni, \n",
    "          - Bir alfanümerik karakterden sonra gelen nokta ve \n",
    "          - Noktanın sonrasında (boşluklar dahil) büyük harf gelme durumu veya\n",
    "          - Yeni satır karakterini baz alır.\n",
    "        Sonrasında cümleler temizlenir ve 1000 karakteri aşmayan parçalara birleştirilir.\n",
    "        \"\"\"\n",
    "        pattern = r'(?<=[a-zA-Z0-9])\\.(?=\\s*[A-Z])|(?<=\\n)'\n",
    "        sentences = re.split(pattern, text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        merged_sentences = []\n",
    "        buffer = \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(buffer) + len(sentence) < 1000:\n",
    "                buffer = f\"{buffer} {sentence}\" if buffer else sentence\n",
    "            else:\n",
    "                if buffer:\n",
    "                    merged_sentences.append(buffer)\n",
    "                buffer = sentence\n",
    "        if buffer:\n",
    "            merged_sentences.append(buffer)\n",
    "        return merged_sentences\n",
    "\n",
    "    def split_sentence_by_word_boundary(self, sentence: str, max_length: int) -> list:\n",
    "        \"\"\"\n",
    "        Bir cümleyi kelime sınırlarına göre böler.\n",
    "        Eğer mevcut parçaya yeni bir kelime eklenince max_length'i aşarsa,\n",
    "        o parçayı listeye ekler ve yeni parçaya kelime eklemeye başlar.\n",
    "        \"\"\"\n",
    "        words = sentence.split()\n",
    "        parts = []\n",
    "        current_part = \"\"\n",
    "        for word in words:\n",
    "            if len(current_part + \" \" + word) > max_length:\n",
    "                parts.append(current_part.strip())\n",
    "                current_part = word\n",
    "            else:\n",
    "                current_part += \" \" + word\n",
    "        if current_part.strip():\n",
    "            parts.append(current_part.strip())\n",
    "        return parts\n",
    "\n",
    "    def preprocess_sentences(self, sentences: list) -> list:\n",
    "        \"\"\"\n",
    "        Cümleleri ön işlemden geçirir:\n",
    "          - Uzunluğu 3 karakterden kısa olanları atlar.\n",
    "          - 10000 karakterden uzun cümleleri kelime bazında bölerek parçalar.\n",
    "          - Diğer cümleleri olduğu gibi ekler.\n",
    "        \"\"\"\n",
    "        processed_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if len(sentence) < 3:\n",
    "                continue\n",
    "            if len(sentence) > 10000:\n",
    "                splits = self.split_sentence_by_word_boundary(sentence, 10000)\n",
    "                processed_sentences.extend(splits)\n",
    "            else:\n",
    "                processed_sentences.append(sentence)\n",
    "        return processed_sentences\n",
    "\n",
    "    def create_embeddings(self, texts: list) -> list:\n",
    "        response = openai.Embedding.create(input=texts, model=self.embedding_model)\n",
    "        return [d[\"embedding\"] for d in response[\"data\"]]\n",
    "\n",
    "    def get_embeddings_batch(self, sentences: list, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Cümleleri 2048'lik partiler halinde işleyerek embedding'leri toplar.\n",
    "        \"\"\"\n",
    "        batch_size = 2048\n",
    "        embeddings_result = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i+batch_size]\n",
    "            batch_result = self.create_embeddings(batch)\n",
    "            if not batch_result:\n",
    "                print(f\"Error: Embedding API returned no data for batch {i // batch_size} and agent {agent_id}\")\n",
    "            embeddings_result.extend(batch_result)\n",
    "        return embeddings_result\n",
    "\n",
    "    def calculate_divergence(self, embedding1: list, embedding2: list) -> float:\n",
    "        \"\"\"\n",
    "        İki embedding arasındaki farkı (divergence) hesaplar.\n",
    "        Hesaplama, kosinüs benzerliği esasına göre yapılır; sonuç 1 - cosine_similarity olarak döner.\n",
    "        \"\"\"\n",
    "        if len(embedding1) != len(embedding2):\n",
    "            return 0.0\n",
    "        dot_product = 0.0\n",
    "        magnitude1 = 0.0\n",
    "        magnitude2 = 0.0\n",
    "        for v1, v2 in zip(embedding1, embedding2):\n",
    "            dot_product += v1 * v2\n",
    "            magnitude1 += v1 ** 2\n",
    "            magnitude2 += v2 ** 2\n",
    "        magnitude1 = math.sqrt(magnitude1)\n",
    "        magnitude2 = math.sqrt(magnitude2)\n",
    "        if magnitude1 == 0 or magnitude2 == 0:\n",
    "            return 0.0\n",
    "        cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "        return 1 - cosine_similarity\n",
    "\n",
    "    def calculate_average_embedding(self, embeddings: list) -> list:\n",
    "        \"\"\"\n",
    "        Bir grup embedding'in ortalamasını hesaplar.\n",
    "        \"\"\"\n",
    "        if not embeddings:\n",
    "            return []\n",
    "        num_embeddings = len(embeddings)\n",
    "        vector_length = len(embeddings[0])\n",
    "        sum_vector = [0.0] * vector_length\n",
    "        for embedding in embeddings:\n",
    "            for i, value in enumerate(embedding):\n",
    "                sum_vector[i] += value\n",
    "        average_vector = [s / num_embeddings for s in sum_vector]\n",
    "        return average_vector\n",
    "\n",
    "    def sliding_window_divergence(self, sentences: list, window_size: int, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Kayan pencere yöntemiyle, belirli pencere boyutundaki cümle gruplarının divergence değerlerini hesaplar.\n",
    "        \"\"\"\n",
    "        divergences = []\n",
    "        embeddings = self.get_embeddings_batch(sentences, agent_id)\n",
    "        half_window = window_size // 2\n",
    "        for i in range(0, len(sentences) - window_size + 1):\n",
    "            window1_embeddings = embeddings[i:i+half_window]\n",
    "            window2_embeddings = embeddings[i+half_window:i+half_window+half_window]\n",
    "            avg_embedding1 = self.calculate_average_embedding(window1_embeddings)\n",
    "            avg_embedding2 = self.calculate_average_embedding(window2_embeddings)\n",
    "            divergence = self.calculate_divergence(avg_embedding1, avg_embedding2)\n",
    "            divergences.append(divergence)\n",
    "        return divergences\n",
    "\n",
    "    def detect_peaks(self, divergences: list, threshold: float) -> list:\n",
    "        \"\"\"\n",
    "        Hesaplanan divergence değerleri arasında, belirlenen eşik değerinin üzerinde ve komşularına göre zirve olan noktaları tespit eder.\n",
    "        \"\"\"\n",
    "        peaks = []\n",
    "        if not divergences:\n",
    "            return peaks\n",
    "        max_divergence = max(divergences)\n",
    "        threshold_value = threshold * max_divergence\n",
    "        for i, value in enumerate(divergences):\n",
    "            prev_val = divergences[i-1] if i - 1 >= 0 else 0\n",
    "            next_val = divergences[i+1] if i + 1 < len(divergences) else 0\n",
    "            if value > threshold_value and value > prev_val and value > next_val:\n",
    "                peaks.append(i)\n",
    "        return peaks\n",
    "\n",
    "    def semantic_chunking(self, text: str, window_size: int, threshold: float, agent_id=\"live\") -> list:\n",
    "        \"\"\"\n",
    "        Metni anlamsal parçalara böler:\n",
    "          1. Metin cümlelere ayrılır.\n",
    "          2. Kayan pencere yöntemiyle divergence değerleri hesaplanır.\n",
    "          3. Tespit edilen peak noktaları, metni bölecek yerler olarak kullanılır.\n",
    "          4. Cümle grupları (chunk) oluşturulur.\n",
    "        \"\"\"\n",
    "        sentences = self.split_text_into_sentences(text)\n",
    "        sentences = self.preprocess_sentences(sentences)\n",
    "        divergences = self.sliding_window_divergence(sentences, window_size, agent_id)\n",
    "        split_points = self.detect_peaks(divergences, threshold)\n",
    "        chunks = []\n",
    "        last_split = 0\n",
    "        for point in split_points:\n",
    "            chunk = \". \".join(sentences[last_split:point])\n",
    "            chunks.append(chunk)\n",
    "            last_split = point\n",
    "        chunk = \". \".join(sentences[last_split:])\n",
    "        chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def create_documents(self, docs: list) -> list:\n",
    "        \"\"\"\n",
    "        Verilen metin (doküman) listesini semantic chunking algoritması ile parçalara ayırır \n",
    "        ve her parçayı basit bir doküman olarak döndürür.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for doc in docs:\n",
    "            chunks = self.semantic_chunking(doc, self.window_size, self.threshold, agent_id=\"live\")\n",
    "            for chunk in chunks:\n",
    "                all_chunks.append({\"page_content\": chunk})\n",
    "        return all_chunks\n",
    "\n",
    "def integrate_pipeline(texts: list, material_ids: list, chunker: SemanticChunker,\n",
    "                       window_size: int = 6, threshold: float = 0.7, long_text_limit: int = 10000) -> tuple:\n",
    "    \"\"\"\n",
    "    Verilen metinleri ve ilgili material ID'lerini kontrol eder.\n",
    "    Eğer metin uzunluğu long_text_limit (default: 10000) karakterden fazlaysa,\n",
    "    semantic_chunking ile anlamsal parçalara ayrılır ve her parçaya orijinal material ID atanır.\n",
    "    Aksi halde, metin doğrudan eklenir.\n",
    "    Son olarak, tüm işlenmiş metinler preprocess_sentences ile tekrar işlenir.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    processed_material_ids = []\n",
    "    \n",
    "    for text, mat_id in zip(texts, material_ids):\n",
    "        if len(text) >= long_text_limit:\n",
    "            chunked_data = chunker.semantic_chunking(text, window_size, threshold, agent_id=\"live\")\n",
    "            processed_data.extend(chunked_data)\n",
    "            processed_material_ids.extend([mat_id] * len(chunked_data))\n",
    "        else:\n",
    "            processed_data.append(text)\n",
    "            processed_material_ids.append(mat_id)\n",
    "    \n",
    "    processed_data = chunker.preprocess_sentences(processed_data)\n",
    "    \n",
    "    return processed_data, processed_material_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG-Qdrant\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "OPENAI_API_KEY = config[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "collection_name = \"jotform-pdf-LLM_data_collection\"\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "\n",
    "def embed_text(text):\n",
    "    return get_embedding(text, embedding_model_name)\n",
    "\n",
    "chunker = SemanticChunker(embedding_model_name, window_size=6, threshold=0.7)\n",
    "\n",
    "pdf_path = \"Foundations of LLM.pdf\"\n",
    "documents = []\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text.append(page_text)\n",
    "    documents = [\"\\n\".join(full_text)]\n",
    "\n",
    "material_ids = list(range(len(documents)))\n",
    "processed_texts, processed_material_ids = integrate_pipeline(documents, material_ids, chunker)\n",
    "all_chunks = [{\"page_content\": text, \"material_id\": mid} for text, mid in zip(processed_texts, processed_material_ids)]\n",
    "\n",
    "vector_size = 3072\n",
    "\n",
    "existing_collections = [c.name for c in qdrant_client.get_collections().collections]\n",
    "if collection_name in existing_collections:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "qdrant_client.create_collection(collection_name=collection_name, vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE))\n",
    "points = []\n",
    "for chunk in all_chunks:\n",
    "    chunk_text = chunk[\"page_content\"].strip()\n",
    "    if not chunk_text:\n",
    "        continue\n",
    "    vector = embed_text(chunk_text)\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\"text\": chunk_text, \"material_id\": chunk[\"material_id\"]}\n",
    "    }\n",
    "    points.append(point)\n",
    "qdrant_client.upsert(collection_name=collection_name, points=points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_direct_qdrant(qdrant_client, collection_name, query):\n",
    "    \"\"\"\n",
    "    Sorgu üzerinden RAG işlemini test eder.\n",
    "    - Sorgunun embedding'ini OpenAI API'si ile oluşturur.\n",
    "    - Qdrant koleksiyonunda benzer metinleri arar.\n",
    "    - Elde edilen kaynaklardan bağlam oluşturur ve ChatCompletion modeline göndererek yanıt alır.\n",
    "    - İnference süresini hesaplar ve çıktıları ekrana yazdırır.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    query_vector = get_embedding(query, embedding_model_name)\n",
    "    \n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [result.payload[\"text\"] for result in search_results]\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        print(\"Qdrant'tan eşleşen belge bulunamadı.\")\n",
    "        return\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    print(\"\\nQdrant'tan Kullanılan Kaynaklar:\")\n",
    "    for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "        print(f\"{idx}. (Lenghth: {len(doc)}) {doc[:100]}...\")\n",
    "        \n",
    "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer: \"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that provides precise answers based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nInference zamanı: {round(end_time - start_time, 3)} saniye\")\n",
    "    print(f\"\\nModelin cevabı:\\n{response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Lenghth: 10000) 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3.2 Training ...\n",
      "2. (Lenghth: 9999) d from scratch A well-known example of this is training sequence models by successively predict ing ...\n",
      "3. (Lenghth: 9999) standard We optimize the model by minimizing the loss over the tuning samples The out come is the op...\n",
      "4. (Lenghth: 7714) After all these events, Tom has 17 marbles We can add these reasoning steps into the prompt, and get...\n",
      "5. (Lenghth: 9998) large language models We will discuss these issues more deeply in Chapter 3 . However, it is worth n...\n",
      "\n",
      "Inference zamanı: 6.016 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Pre-training is the process of optimizing a neural network on a large dataset before it is further trained or fine-tuned for specific tasks. This approach leverages the idea that a model trained on a general task can be adapted to perform well on various downstream tasks, reducing the need for extensive labeled data. Pre-training can be conducted using unsupervised, supervised, or self-supervised learning methods. In the context of natural language processing (NLP), pre-training often involves training models on vast amounts of unlabeled text data to learn general language representations, which can then be fine-tuned or adapted to specific applications, such as text classification or translation.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Which types of models are widely used in NLP pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Lenghth: 9999) standard We optimize the model by minimizing the loss over the tuning samples The out come is the op...\n",
      "2. (Lenghth: 9999) d from scratch A well-known example of this is training sequence models by successively predict ing ...\n",
      "3. (Lenghth: 10000) 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3.2 Training ...\n",
      "4. (Lenghth: 9992) [CLS] It is raining . [SEP] I need an hat. [SEP] This helps the model learn to recover a token from ...\n",
      "5. (Lenghth: 7714) After all these events, Tom has 17 marbles We can add these reasoning steps into the prompt, and get...\n",
      "\n",
      "Inference zamanı: 3.911 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The two major types of models widely used in NLP pre-training are:\n",
      "\n",
      "1. **Sequence Encoding Models**: These models represent a sequence of words or tokens as either a real-valued vector or a sequence of vectors. They are typically used as input to another model, such as a sentence classification system.\n",
      "\n",
      "2. **Sequence Generation Models**: These models are designed to generate a sequence of tokens based on a given context. They are often employed independently to address language generation problems, such as question answering and machine translation.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Which types of models are widely used in NLP pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How do we implement permuted language modelling?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Lenghth: 9998) large language models We will discuss these issues more deeply in Chapter 3 . However, it is worth n...\n",
      "2. (Lenghth: 9997) BERT is used to illus trate how sequence models are pre- trained via masked language modeling and ap...\n",
      "3. (Lenghth: 10000) token given its previous context tokens This token prediction task can be described as ˆxi= arg max ...\n",
      "4. (Lenghth: 10000) express other tasks in the same way For exam ple [CLS] Answer: when was Albert Einstein born? → /a\\}...\n",
      "5. (Lenghth: 7714) After all these events, Tom has 17 marbles We can add these reasoning steps into the prompt, and get...\n",
      "\n",
      "Inference zamanı: 5.382 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Permuted language modeling is implemented by rearranging the order of tokens in a sequence and then training the model to predict the tokens based on this permuted input. Here’s a step-by-step outline of the process:\n",
      "\n",
      "1. **Input Preparation**: Start with a sequence of tokens, for example, \\( x = [x_0, x_1, x_2, \\ldots, x_m] \\).\n",
      "\n",
      "2. **Permutation**: Randomly permute the order of the tokens in the sequence to create a new sequence \\( \\tilde{x} \\). This means that the positions of the tokens are shuffled, but the tokens themselves remain unchanged.\n",
      "\n",
      "3. **Model Training**: Train the model to predict the original sequence \\( x \\) from the permuted sequence \\( \\tilde{x} \\). The model learns to leverage the context provided by the other tokens in the permuted sequence to reconstruct the original order.\n",
      "\n",
      "4. **Loss Function**: Use a suitable loss function, such as cross-entropy loss, to measure the difference between the predicted tokens and the original tokens at their respective positions.\n",
      "\n",
      "5. **Optimization**: Optimize the model parameters to minimize the loss across the training dataset, which consists of many such permuted sequences.\n",
      "\n",
      "6. **Inference**: During inference, the model can generate or reconstruct sequences based on the learned relationships between tokens, even when they are presented in a non-sequential order.\n",
      "\n",
      "This approach allows the model to learn bidirectional context\n"
     ]
    }
   ],
   "source": [
    "query3 = \"How do we implement permuted language modelling?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the large-scale pre-training of the document?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Lenghth: 7714) After all these events, Tom has 17 marbles We can add these reasoning steps into the prompt, and get...\n",
      "2. (Lenghth: 10000) 4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173 4.3.2 Training ...\n",
      "3. (Lenghth: 9999) standard We optimize the model by minimizing the loss over the tuning samples The out come is the op...\n",
      "4. (Lenghth: 9999) d from scratch A well-known example of this is training sequence models by successively predict ing ...\n",
      "5. (Lenghth: 9993) can handle multiple languages using a single model While th is approach shows strong abilities in mu...\n",
      "\n",
      "Inference zamanı: 4.138 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Large-scale pre-training in the context of the document refers to the process of training large language models (LLMs) on vast amounts of data to enhance their ability to understand and generate human language. This involves using self-supervised learning techniques where models learn from unlabelled data by predicting parts of the input data based on other parts. The objective is to maximize the likelihood of the data, which is typically achieved through gradient descent. \n",
      "\n",
      "The document highlights that effective pre-training requires not only large datasets but also high-quality and diverse data to ensure that the models can generalize well across various tasks. It discusses the importance of data preparation, including filtering and cleaning to improve data quality, and emphasizes the need for diverse datasets to cover different types of tasks and languages. \n",
      "\n",
      "Additionally, the document outlines the challenges associated with training large models, such as instability during training and the need for sophisticated techniques to manage these issues. It also mentions the significance of distributed training systems to handle the computational demands of training LLMs at scale. Overall, large-scale pre-training is crucial for developing models that can perform a wide range of natural language processing tasks effectively.\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is the large-scale pre-training of the document?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
