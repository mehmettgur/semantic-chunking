{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import openai\n",
    "import PyPDF2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "def get_embedding(text, model):\n",
    "        \"\"\"\n",
    "        Verilen metin için OpenAI API'sini kullanarak embedding üretir.\n",
    "\n",
    "        \"\"\"\n",
    "        response = openai.Embedding.create(input=[text], model=model)\n",
    "        return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "class SemanticChunking:\n",
    "    def __init__(self, embedding_model, window_size, transfer_sentence_count):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.window_size = window_size\n",
    "        self.transfer_sentence_count = transfer_sentence_count\n",
    "\n",
    "    def split_segment_by_word_boundary(self, segment, max_length):\n",
    "        \"\"\"\n",
    "        Bir segmenti, maksimum karakter uzunluğunu aşmayacak şekilde kelime sınırlarına göre parçalara böler.\n",
    "\n",
    "        \"\"\"\n",
    "        words = segment.split()\n",
    "        parts = []\n",
    "        current_part = \"\"\n",
    "        for word in words:\n",
    "            if len(current_part) + len(word) + 1 > max_length:\n",
    "                parts.append(current_part.strip())\n",
    "                current_part = word\n",
    "            else:\n",
    "                current_part = f\"{current_part} {word}\" if current_part else word\n",
    "        if current_part.strip():\n",
    "            parts.append(current_part.strip())\n",
    "        return parts\n",
    "\n",
    "    def preprocess_segments(self, segments):\n",
    "        \"\"\"\n",
    "        Segmentleri ön işler; boşlukları temizler, kısa segmentleri atar ve 10000 karakteri aşan segmentleri kelime bazında böler.\n",
    "\n",
    "        \"\"\"\n",
    "        processed = []\n",
    "        for seg in segments:\n",
    "            seg = seg.strip()\n",
    "            if len(seg) < 3:\n",
    "                continue\n",
    "            if len(seg) > 10000:\n",
    "                processed.extend(self.split_segment_by_word_boundary(seg, 10000))\n",
    "            else:\n",
    "                processed.append(seg)\n",
    "        return processed\n",
    "\n",
    "    def split_text_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        Metni, noktalama işaretleri ve yeni satır karakterlerini baz alarak cümlelere böler.\n",
    "        Daha sonra cümleler, yaklaşık 750 karakterlik parçalarda birleştirilir.\n",
    "\n",
    "        \"\"\"\n",
    "        pattern = r'(?<=[a-zA-Z0-9])([.!])(?=\\s*[A-Z])|(?<=\\n)' \n",
    "        temp_parts = re.split(pattern, text)\n",
    "        temp_parts = [part if part is not None else \"\" for part in temp_parts]\n",
    "        reattached_sentences = []\n",
    "        i = 0\n",
    "        while i < len(temp_parts):\n",
    "            chunk = temp_parts[i]\n",
    "            if i + 1 < len(temp_parts) and re.match(r'^[.!]$', temp_parts[i+1]):\n",
    "                chunk += temp_parts[i+1]\n",
    "                i += 1\n",
    "            chunk = chunk.strip()\n",
    "            if chunk:\n",
    "                reattached_sentences.append(chunk)\n",
    "            i += 1\n",
    "\n",
    "        merged_sentences = []\n",
    "        buffer = \"\"\n",
    "        for sentence in reattached_sentences:\n",
    "            # pdf uzunluğuna bağlı olarak 750 karakter limiti değiştirilebilir.\n",
    "            if len(buffer) + len(sentence) < 750:\n",
    "                buffer = f\"{buffer} {sentence}\" if buffer else sentence\n",
    "            else:\n",
    "                if buffer:\n",
    "                    merged_sentences.append(buffer)\n",
    "                buffer = sentence\n",
    "        if buffer:\n",
    "            merged_sentences.append(buffer)\n",
    "        return merged_sentences\n",
    "\n",
    "    def rule_based_segmentation(self, text):\n",
    "        \"\"\"\n",
    "        Metni, cümlelere ayırıp ön işlemden geçirerek kural tabanlı segmentlere böler.\n",
    "\n",
    "        \"\"\"\n",
    "        segments = self.split_text_into_sentences(text)\n",
    "        segments = self.preprocess_segments(segments)\n",
    "        return segments\n",
    "    \n",
    "\n",
    "    def create_embeddings(self, texts: list) -> list:\n",
    "        response = openai.Embedding.create(input=texts, model=self.embedding_model)\n",
    "        return [d[\"embedding\"] for d in response[\"data\"]]\n",
    "\n",
    "\n",
    "    def calculate_dynamic_threshold_from_divergences(self, divergences):\n",
    "        \"\"\"\n",
    "        Verilen divergence değerlerine göre her window için dinamik bir threshold hesaplar.\n",
    "        - Divergence'ların ortalaması ve standart sapması hesaplanır.\n",
    "        - Standart sapmanın küçük, orta veya büyük olmasına bağlı olarak farklı faktörler uygulanır.\n",
    "\n",
    "        \"\"\"\n",
    "        mean_div = sum(divergences) / len(divergences)\n",
    "        variance = sum((d - mean_div) ** 2 for d in divergences) / len(divergences)\n",
    "        std_div = math.sqrt(variance)\n",
    "        if std_div < 0.1:\n",
    "            factor = 1.4  # buradaki çarpımlar değiştirilebilir\n",
    "        elif std_div > 0.3:\n",
    "            factor = 1.0\n",
    "        else:\n",
    "            factor = 1.2\n",
    "        return mean_div + std_div * factor\n",
    "\n",
    "    def semantic_merging(self, segments):\n",
    "        \"\"\"\n",
    "        Segmentleri, her pencere içerisindeki embedding'ler arası divergence hesaplanarak semantik olarak birleştirir.\n",
    "        Her window için belirlenen dinamik threshold değerine göre bölme noktaları tespit edilir ve\n",
    "        bu noktalara göre segmentler birleştirilir.\n",
    "\n",
    "        \"\"\"\n",
    "        n = len(segments)\n",
    "        if n < self.window_size:\n",
    "            return [\" \".join(segments)]\n",
    "        \n",
    "        embeddings = self.create_embeddings(segments)\n",
    "        split_points = set()\n",
    "        \n",
    "        for window_start in range(0, n - self.window_size + 1):\n",
    "            window_end = window_start + self.window_size\n",
    "            window_embeddings = embeddings[window_start:window_end]\n",
    "            window_divergences = []\n",
    "            for i in range(self.window_size - 1):\n",
    "                sim = cosine_similarity([window_embeddings[i]], [window_embeddings[i+1]])[0][0]\n",
    "                divergence = 1 - sim\n",
    "                window_divergences.append(divergence)\n",
    "            local_threshold = self.calculate_dynamic_threshold_from_divergences(window_divergences)\n",
    "            \n",
    "            for i, div in enumerate(window_divergences):\n",
    "                if div > local_threshold:\n",
    "                    global_index = window_start + i + 1\n",
    "                    split_points.add(global_index)\n",
    "        \n",
    "        split_points = sorted(list(split_points))\n",
    "        chunks = []\n",
    "        last_split = 0\n",
    "        for point in split_points:\n",
    "            chunk = \" \".join(segments[last_split:point])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            last_split = point\n",
    "        if last_split < n:\n",
    "            chunk = \" \".join(segments[last_split:])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def adjust_boundaries(self, chunks):\n",
    "        \"\"\"\n",
    "        Chunk'lar arasındaki sınırları ayarlamak için kullanılır.\n",
    "        - Her iki chunk arasındaki geçiş bölgesinde, belirli sayıda cümlenin transferi ile\n",
    "          daha uyumlu bir sınır elde edilmesi amaçlanır.\n",
    "        - Bir sonraki chunk'ın ilk 'transfer_sentence_count' cümlesi aday olarak alınır.\n",
    "        - Aday metnin, önceki chunk ve kalan kısmıyla olan benzerliği karşılaştırılarak,\n",
    "          eğer önceki chunk ile olan benzerlik daha yüksekse, aday cümleler önceki chunk'a eklenir.\n",
    "\n",
    "        \"\"\"\n",
    "        adjusted_chunks = chunks.copy()\n",
    "        candidate_texts = []\n",
    "        previous_texts = []\n",
    "        remainder_texts = []\n",
    "        indices = []\n",
    "        \n",
    "        for i in range(len(adjusted_chunks) - 1):\n",
    "            next_sentences = self.split_text_into_sentences(adjusted_chunks[i+1])\n",
    "            if not next_sentences or len(next_sentences) <= self.transfer_sentence_count:\n",
    "                continue\n",
    "            candidate_text = \" \".join(next_sentences[:self.transfer_sentence_count])\n",
    "            remainder = \" \".join(next_sentences[self.transfer_sentence_count:])\n",
    "            candidate_texts.append(candidate_text)\n",
    "            previous_texts.append(adjusted_chunks[i])\n",
    "            remainder_texts.append(remainder)\n",
    "            indices.append(i)\n",
    "        \n",
    "        if candidate_texts:\n",
    "            candidate_embeddings = self.create_embeddings(candidate_texts)\n",
    "            previous_embeddings = self.create_embeddings(previous_texts)\n",
    "            remainder_embeddings = self.create_embeddings(remainder_texts)\n",
    "        \n",
    "            for idx, i in enumerate(indices):\n",
    "                candidate_emb = candidate_embeddings[idx]\n",
    "                prev_emb = previous_embeddings[idx]\n",
    "                next_emb = remainder_embeddings[idx]\n",
    "                sim_prev = cosine_similarity([prev_emb], [candidate_emb])[0][0]\n",
    "                sim_next = cosine_similarity([next_emb], [candidate_emb])[0][0]\n",
    "                \n",
    "                if sim_prev > sim_next:\n",
    "                    next_sentences = self.split_text_into_sentences(adjusted_chunks[i+1])\n",
    "                    candidate_text = \" \".join(next_sentences[:self.transfer_sentence_count])\n",
    "                    adjusted_chunks[i] = adjusted_chunks[i].strip() + \" \" + candidate_text\n",
    "                    adjusted_chunks[i+1] = \" \".join(next_sentences[self.transfer_sentence_count:])\n",
    "        return adjusted_chunks\n",
    "\n",
    "    def create_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Verilen metinleri, kural tabanlı segmentasyon, semantik birleştirme, sınır ayarlaması ve\n",
    "        gerektiğinde uzun chunk'ların kelime sınırına göre bölünmesi adımlarından geçirerek dokümanlar oluşturur.\n",
    "\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for text in texts:\n",
    "            segments = self.rule_based_segmentation(text)\n",
    "            initial_chunks = self.semantic_merging(segments)\n",
    "            adjusted_chunks = self.adjust_boundaries(initial_chunks)\n",
    "            final_chunks = []\n",
    "            for chunk in adjusted_chunks:\n",
    "                if len(chunk) > 10000:\n",
    "                    final_chunks.extend(self.split_segment_by_word_boundary(chunk, 10000))\n",
    "                else:\n",
    "                    final_chunks.append(chunk)\n",
    "            all_chunks.extend(final_chunks)\n",
    "        return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid-2-pdf-LLM-scipy_data_collection koleksiyonu silindi, yeniden oluşturuluyor.\n",
      "hybrid-2-pdf-LLM-scipy_data_collection koleksiyonu başarıyla oluşturuldu.\n",
      "217 adet semantic chunk başarıyla Qdrant koleksiyonuna eklendi.\n"
     ]
    }
   ],
   "source": [
    "# RAG ve Qdrant işlemleri\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "OPENAI_API_KEY = config[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "collection_name = \"hybrid-2-pdf-LLM-scipy_data_collection\"\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "\n",
    "chunker = SemanticChunking(\n",
    "    embedding_model=embedding_model_name,  \n",
    "    window_size=6,  \n",
    "    transfer_sentence_count=2  # değiştirilebilir\n",
    ")\n",
    "\n",
    "pdf_path = \"Foundations of LLM.pdf\"\n",
    "documents = []\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text.append(page_text)\n",
    "    documents = [\"\\n\".join(full_text)]\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunker.create_documents([doc])\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "vector_size = 3072\n",
    "\n",
    "existing_collections = [c.name for c in qdrant_client.get_collections().collections]\n",
    "if collection_name in existing_collections:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"{collection_name} koleksiyonu silindi, yeniden oluşturuluyor.\")\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    ")\n",
    "print(f\"{collection_name} koleksiyonu başarıyla oluşturuldu.\")\n",
    "\n",
    "points = []\n",
    "for chunk in all_chunks:\n",
    "    vector = get_embedding(chunk, embedding_model_name)\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\"text\": chunk}\n",
    "    }\n",
    "    points.append(point)\n",
    "\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "print(f\"{len(points)} adet semantic chunk başarıyla Qdrant koleksiyonuna eklendi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_direct_qdrant(qdrant_client, collection_name, query):\n",
    "    \"\"\"\n",
    "    Sorgu üzerinden RAG işlemini test eder.\n",
    "    - Sorgunun embedding'ini OpenAI API'si ile oluşturur.\n",
    "    - Qdrant koleksiyonunda benzer metinleri arar.\n",
    "    - Elde edilen kaynaklardan bağlam oluşturur ve ChatCompletion modeline göndererek yanıt alır.\n",
    "    - İnference süresini hesaplar ve çıktıları ekrana yazdırır.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    query_vector = get_embedding(query, embedding_model_name)\n",
    "    \n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [result.payload[\"text\"] for result in search_results]\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        print(\"Qdrant'tan eşleşen belge bulunamadı.\")\n",
    "        return\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    print(\"\\nQdrant'tan Kullanılan Kaynaklar:\")\n",
    "    for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "        print(f\"{idx}. (Lenghth: {len(doc)}) {doc[:100]}...\")\n",
    "        \n",
    "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer: \"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that provides precise answers based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nInference zamanı: {round(end_time - start_time, 3)} saniye\")\n",
    "    print(f\"\\nModelin cevabı:\\n{response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Lenghth: 4031) In this section, we discuss the basic ideas in addressing the se issues. 1.1.1 Unsupervised, Supervi...\n",
      "2. (Lenghth: 3053) Such a met hod has been successfully used in sev- eral NLP areas, such as word sense disambiguation ...\n",
      "3. (Lenghth: 4191) 4.4.4 Step-by-step Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 198 4.4.5 Inference...\n",
      "4. (Lenghth: 2126) ample, we randomly choose one of them for each training sampl e. In practice, the outcome of encoder...\n",
      "5. (Lenghth: 4906) different problems together, with the beneﬁt of training a s ingle model that can perform many tasks...\n",
      "\n",
      "Inference zamanı: 3.477 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Pre-training refers to the process of optimizing a neural network before it is further trained or tuned for specific tasks. It is based on the assumption that a model pre-trained on one task can be adapted to perform another task, reducing the need for extensive labeled data for complex neural networks. Pre-training can be conducted using three main approaches: unsupervised learning, where the model learns from unlabeled data; supervised learning, where the model is trained on labeled tasks; and self-supervised learning, where the model generates its own supervision signals from unlabeled data. This process enables the development of more general models that can be fine-tuned for various downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Which types of models are widely used in NLP pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Lenghth: 3053) Such a met hod has been successfully used in sev- eral NLP areas, such as word sense disambiguation ...\n",
      "2. (Lenghth: 2186) As an example, BERT is used to illus trate how sequence models are pre- trained via masked language ...\n",
      "3. (Lenghth: 4834) This is often achieved via dynamic programming, which, in th e context of path ﬁnding over a lattice...\n",
      "4. (Lenghth: 3459) goals and methods of language modeling have remained largel y unchanged over the decades since then....\n",
      "5. (Lenghth: 4191) 4.4.4 Step-by-step Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 198 4.4.5 Inference...\n",
      "\n",
      "Inference zamanı: 3.25 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The two major types of models widely used in NLP pre-training are:\n",
      "\n",
      "1. **Sequence Encoding Models**: These models represent a sequence of words or tokens as either a real-valued vector or a sequence of vectors, which is typically used as input to another model, such as a sentence classification system.\n",
      "\n",
      "2. **Sequence Generation Models**: These models refer to the problem of generating a sequence of tokens based on a given context, which can involve tasks such as machine translation, summarization, question answering, and dialogue generation.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Which types of models are widely used in NLP pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How do we implement permuted language modelling?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Lenghth: 5429) sense to predict any of the tokens in this sequence. 1.2.2.1 Masked Language Modeling One of the mos...\n",
      "2. (Lenghth: 2895) ceive a sequence of tokens x0,...,x i−1and produce a distribution over the vocabulary V(de- noted by...\n",
      "3. (Lenghth: 1439) ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ encoding: h0h1h2 h3h4h5h6h7h8 h9 h10h11 ↓ Softmax ↓ Is Next or Not? 1.2 Self...\n",
      "4. (Lenghth: 3459) goals and methods of language modeling have remained largel y unchanged over the decades since then....\n",
      "5. (Lenghth: 3090) number of tasks and generalize to perform new tasks with a sma ll adaptation effort [ Bubeck et al. ...\n",
      "\n",
      "Inference zamanı: 7.857 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Permuted language modeling involves making sequential predictions of tokens in a non-linear order, allowing for predictions to occur in any sequence rather than strictly following the natural order of the text. Here’s how to implement permuted language modeling:\n",
      "\n",
      "1. **Input Preparation**: Start with a sequence of tokens, such as \\( x_0, x_1, x_2, x_3, x_4 \\).\n",
      "\n",
      "2. **Define Prediction Order**: Determine a new order for predicting the tokens. For example, instead of predicting in the order \\( x_0 \\to x_1 \\to x_2 \\to x_3 \\to x_4 \\), you might choose \\( x_0 \\to x_4 \\to x_2 \\to x_1 \\to x_3 \\).\n",
      "\n",
      "3. **Model Architecture**: Use a language model architecture, typically a Transformer, that can handle the input sequence. Each token in the sequence is represented by an embedding that combines the token embedding and positional embedding.\n",
      "\n",
      "4. **Training Process**:\n",
      "   - For each token in the defined prediction order, the model predicts the token based on the embeddings of the tokens that have been processed so far.\n",
      "   - The model outputs a probability distribution over the vocabulary for each token position based on the previous tokens in the sequence.\n",
      "\n",
      "5. **Loss Calculation**: During training, compute the loss using the predicted probabilities for the actual tokens in the sequence. This is typically done using cross-\n"
     ]
    }
   ],
   "source": [
    "query3 = \"How do we implement permuted language modelling?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the large-scale pre-training of the document?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Lenghth: 4031) In this section, we discuss the basic ideas in addressing the se issues. 1.1.1 Unsupervised, Supervi...\n",
      "2. (Lenghth: 6359) The training task is itself standard: the objective is to max imize the likelihood, which can be ach...\n",
      "3. (Lenghth: 3053) Such a met hod has been successfully used in sev- eral NLP areas, such as word sense disambiguation ...\n",
      "4. (Lenghth: 2186) As an example, BERT is used to illus trate how sequence models are pre- trained via masked language ...\n",
      "5. (Lenghth: 4191) 4.4.4 Step-by-step Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 198 4.4.5 Inference...\n",
      "\n",
      "Inference zamanı: 6.226 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Large-scale pre-training refers to the process of training neural networks, particularly in natural language processing (NLP), on vast amounts of unlabeled data using self-supervised learning techniques. This approach allows models to learn general language understanding and generation capabilities without the need for extensive labeled datasets. The pre-trained models can then be fine-tuned or adapted to specific downstream tasks, significantly reducing the reliance on task-specific labeled data. \n",
      "\n",
      "Key aspects of large-scale pre-training include:\n",
      "\n",
      "1. **Data Requirements**: Large language models (LLMs) require trillions of tokens for pre-training, which is orders of magnitude larger than conventional NLP models. The quality, diversity, and bias of the training data are crucial factors that affect model performance.\n",
      "\n",
      "2. **Self-Supervised Learning**: This method involves training models using supervision signals generated from the data itself, such as predicting masked words in a text. This enables the model to learn from unlabeled data effectively.\n",
      "\n",
      "3. **Adaptability**: Pre-trained models serve as foundation models that can be easily adapted to various tasks through fine-tuning or prompting, allowing for efficient transfer of learned knowledge.\n",
      "\n",
      "4. **Challenges**: Large-scale pre-training presents challenges such as data quality issues, model stability during training, and privacy concerns regarding sensitive information in the training data.\n",
      "\n",
      "Overall, large-scale pre-training has transformed the NLP landscape by enabling the development of powerful models that can generalize across multiple tasks without extensive retraining.\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is the large-scale pre-training of the document?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
