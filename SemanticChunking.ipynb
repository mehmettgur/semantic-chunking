{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HScipyHybridSemanticChunker2:\n",
    "    def __init__(self, embeddings, similarity_threshold, cluster_threshold, window_size, transfer_sentence_count, debug=False):\n",
    "        \"\"\"\n",
    "        :param embeddings: embed_query metoduna sahip bir embedding modeli (örn: OpenAIEmbeddings)\n",
    "        :param similarity_threshold: Sliding window içindeki segmentlerin benzerlik eşiği (bu parametre artık referans olarak kalıyor; gerçek karar dinamik threshold ile veriliyor)\n",
    "        :param cluster_threshold: Hiyerarşik kümeleme için benzerlik eşiği\n",
    "        :param window_size: Sliding window'da kaç segmentin bir arada değerlendirileceği (aynı zamanda pencere boyutu)\n",
    "        :param delimiters: Kural tabanlı bölme için kullanılacak regex paterni\n",
    "        :param transfer_sentence_count: Lookahead yöntemiyle sınırdaki kaç cümlenin transferine bakılacağı\n",
    "        :param debug: Debug çıktıları için True yapıldığında ek bilgiler yazdırılır.\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.cluster_threshold = cluster_threshold\n",
    "        self.window_size = window_size\n",
    "        self.transfer_sentence_count = transfer_sentence_count\n",
    "        self.debug = debug\n",
    "\n",
    "    def split_segment_by_word_boundary(self, segment, max_length):\n",
    "        words = segment.split()\n",
    "        parts = []\n",
    "        current_part = \"\"\n",
    "        for word in words:\n",
    "            if len(current_part) + len(word) + 1 > max_length:\n",
    "                parts.append(current_part.strip())\n",
    "                current_part = word\n",
    "            else:\n",
    "                current_part = f\"{current_part} {word}\" if current_part else word\n",
    "        if current_part.strip():\n",
    "            parts.append(current_part.strip())\n",
    "        return parts\n",
    "\n",
    "    def preprocess_segments(self, segments):\n",
    "        processed = []\n",
    "        for seg in segments:\n",
    "            seg = seg.strip()\n",
    "            if len(seg) < 3:\n",
    "                continue\n",
    "            if len(seg) > 10000:\n",
    "                processed.extend(self.split_segment_by_word_boundary(seg, 19000))\n",
    "            else:\n",
    "                processed.append(seg)\n",
    "        return processed\n",
    "\n",
    "    def split_text_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"Metni cümlelere böler.\"\"\"\n",
    "        \n",
    "        pattern = r'(?<=[a-zA-Z0-9])([.!])(?=\\s*[A-Z])|(?<=\\n)'\n",
    "        temp_parts = re.split(pattern, text)\n",
    "        \n",
    "        # None değerleri boş stringe\n",
    "        temp_parts = [part if part is not None else \"\" for part in temp_parts]\n",
    "        \n",
    "        # punc re-attach\n",
    "        reattached_sentences = []\n",
    "        i = 0\n",
    "        while i < len(temp_parts):\n",
    "            chunk = temp_parts[i]\n",
    "            # Sonraki eleman sadece noktalama işareti ise bunu bu chunk'a ekle.\n",
    "            if i + 1 < len(temp_parts) and re.match(r'^[.?!]$', temp_parts[i+1]):\n",
    "                chunk += temp_parts[i+1]\n",
    "                i += 1\n",
    "            chunk = chunk.strip()\n",
    "            if chunk:\n",
    "                reattached_sentences.append(chunk)\n",
    "            i += 1\n",
    "\n",
    "        print(f\"[DEBUG] split_text_into_sentences ile {len(reattached_sentences)} cümle bulundu: {reattached_sentences}\")\n",
    "\n",
    "        # Mevcut merging mantığı (1000 karakteri aşmayacak şekilde birleştirme)\n",
    "        merged_sentences = []\n",
    "        buffer = \"\"\n",
    "        for sentence in reattached_sentences:\n",
    "            if len(buffer) + len(sentence) < 1000:\n",
    "                buffer = f\"{buffer} {sentence}\" if buffer else sentence\n",
    "            else:\n",
    "                if buffer:\n",
    "                    merged_sentences.append(buffer)\n",
    "                buffer = sentence\n",
    "\n",
    "        if buffer:\n",
    "            merged_sentences.append(buffer)\n",
    "\n",
    "        return merged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_segmentation(self, text):\n",
    "        # Önce metni cümlelere bölüp, sonra preprocess uygularız.\n",
    "        segments = self.split_text_into_sentences(text)\n",
    "        segments = self.preprocess_segments(segments)\n",
    "        return segments\n",
    "\n",
    "    def create_embeddings(self, texts: list) -> list:\n",
    "        \"\"\"\n",
    "        Batch embedding metodu:\n",
    "        Tüm metin parçalarını tek seferde embed_documents ile işleyerek performansı artırır.\n",
    "        \"\"\"\n",
    "        return self.embeddings.embed_documents(texts)\n",
    "\n",
    "    def calculate_dynamic_threshold_from_divergences(self, divergences):\n",
    "        if not divergences:\n",
    "            return 0.5\n",
    "        mean_div = sum(divergences) / len(divergences)\n",
    "        variance = sum((d - mean_div) ** 2 for d in divergences) / len(divergences)\n",
    "        std_div = math.sqrt(variance)\n",
    "        if std_div < 0.1:\n",
    "            factor = 1.5\n",
    "        elif std_div > 0.3:\n",
    "            factor = 1.0\n",
    "        else:\n",
    "            factor = 1.25\n",
    "        return mean_div + std_div * factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_merging(self, segments):\n",
    "        \"\"\"\n",
    "        Bölünme mantığı:\n",
    "          - Tüm segmentler üzerinde adım adım kayan pencere uygulanır (step = 1).\n",
    "          - Her pencere için adjacent segmentler arası divergence değerleri hesaplanır.\n",
    "          - Her pencere için yerel dinamik threshold belirlenir.\n",
    "          - Eğer pencere içinde herhangi bir divergence, yerel threshold'u aşıyorsa,\n",
    "            ilgili global index \"split point\" olarak kaydedilir.\n",
    "          - Sonrasında split point'lere göre segmentler birleştirilerek chunk'lar oluşturulur.\n",
    "        \"\"\"\n",
    "        n = len(segments)\n",
    "        if n < self.window_size:\n",
    "            if self.debug:\n",
    "                print(\"[DEBUG] Segment sayısı pencere boyutundan küçük, tüm metin tek chunk olarak alınıyor.\")\n",
    "            return [\" \".join(segments)]\n",
    "        \n",
    "        # Batch embedding: tüm segmentler için embedding'leri tek seferde alıyoruz.\n",
    "        embeddings = self.create_embeddings(segments)\n",
    "        split_points = set()\n",
    "        \n",
    "        # Kayan pencere: adım 1\n",
    "        for window_start in range(0, n - self.window_size + 1):\n",
    "            window_end = window_start + self.window_size\n",
    "            window_embeddings = embeddings[window_start:window_end]\n",
    "            window_divergences = []\n",
    "            for i in range(self.window_size - 1):\n",
    "                sim = cosine_similarity([window_embeddings[i]], [window_embeddings[i+1]])[0][0]\n",
    "                divergence = 1 - sim\n",
    "                window_divergences.append(divergence)\n",
    "            local_threshold = self.calculate_dynamic_threshold_from_divergences(window_divergences)\n",
    "            \n",
    "            if self.debug:\n",
    "                print(f\"\\n[DEBUG] Pencere {window_start}-{window_end} segmentleri: {segments[window_start:window_end]}\")\n",
    "                print(f\"[DEBUG] Divergence değerleri: {window_divergences}\")\n",
    "                print(f\"[DEBUG] Yerel dinamik threshold: {local_threshold:.4f}\")\n",
    "            \n",
    "            # Pencere içindeki her adjacent divergence için kontrol\n",
    "            for i, div in enumerate(window_divergences):\n",
    "                if div > local_threshold:\n",
    "                    global_index = window_start + i + 1\n",
    "                    split_points.add(global_index)\n",
    "                    if self.debug:\n",
    "                        print(f\"[DEBUG] Peak bulundu: Global index {global_index} (divergence: {div:.4f})\")\n",
    "        \n",
    "        split_points = sorted(list(split_points))\n",
    "        if self.debug:\n",
    "            print(f\"\\n[DEBUG] Tespit edilen global split noktaları: {split_points}\")\n",
    "        \n",
    "        # Split noktalarına göre chunk'ları oluştur\n",
    "        chunks = []\n",
    "        last_split = 0\n",
    "        for point in split_points:\n",
    "            chunk = \" \".join(segments[last_split:point])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "                if self.debug:\n",
    "                    print(f\"[DEBUG] Chunk oluşturuldu: start={last_split}, end={point}, ilk 100 karakter: {chunk[:100]}...\")\n",
    "            last_split = point\n",
    "        # Kalan parçayı ekle\n",
    "        if last_split < n:\n",
    "            chunk = \" \".join(segments[last_split:])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "                if self.debug:\n",
    "                    print(f\"[DEBUG] Son Chunk: start={last_split}, end={n}, ilk 100 karakter: {chunk[:100]}...\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_boundaries(self, chunks):\n",
    "        \"\"\"\n",
    "        Batch yöntemiyle boundary adjustment:\n",
    "        Her bir chunk çifti için, eğer sonraki chunk'ta (i+1) transfer_sentence_count'dan fazla cümle bulunuyorsa,\n",
    "        sonraki chunk'ın ilk transfer_sentence_count cümlesi aday segment (candidate) olarak belirlenir,\n",
    "        ve geri kalan kısmı (remainder) olarak alınır. Ardından, bu aday segmentin önceki chunk (i) ile olan benzerliği \n",
    "        (önceki chunk'in embedding'i ile karşılaştırılarak) ile aday segmentin remainder kısmı ile olan benzerliği \n",
    "        karşılaştırılır. Eğer aday segment, önceki chunk ile daha yüksek bir benzerlik (cosine similarity) gösteriyorsa,\n",
    "        bu transfer_sentence_count cümle, sonraki chunk'tan kesilerek önceki chunk'e eklenir.\n",
    "        \"\"\"\n",
    "        adjusted_chunks = chunks.copy()\n",
    "        candidate_texts = []\n",
    "        previous_texts = []\n",
    "        remainder_texts = []\n",
    "        indices = []\n",
    "        \n",
    "        # Her boundary için gerekli metinleri topla.\n",
    "        for i in range(len(adjusted_chunks) - 1):\n",
    "            next_sentences = self.split_text_into_sentences(adjusted_chunks[i+1])\n",
    "            if not next_sentences or len(next_sentences) <= self.transfer_sentence_count:\n",
    "                if self.debug:\n",
    "                    print(f\"[DEBUG] Chunk {i+1} sadece {len(next_sentences)} cümle içeriyor, boundary adjustment atlanıyor.\")\n",
    "                continue\n",
    "            candidate_text = \" \".join(next_sentences[:self.transfer_sentence_count])\n",
    "            remainder = \" \".join(next_sentences[self.transfer_sentence_count:])\n",
    "            candidate_texts.append(candidate_text)\n",
    "            previous_texts.append(adjusted_chunks[i])\n",
    "            remainder_texts.append(remainder)\n",
    "            indices.append(i)\n",
    "        \n",
    "        # Eğer batch için toplanan veri varsa, toplu embed işlemi yap.\n",
    "        if candidate_texts:\n",
    "            candidate_embeddings = self.create_embeddings(candidate_texts)\n",
    "            previous_embeddings = self.create_embeddings(previous_texts)\n",
    "            remainder_embeddings = self.create_embeddings(remainder_texts)\n",
    "        \n",
    "            # Her boundary için cosine similarity hesaplamalarını gerçekleştir.\n",
    "            for idx, i in enumerate(indices):\n",
    "                candidate_emb = candidate_embeddings[idx]\n",
    "                prev_emb = previous_embeddings[idx]\n",
    "                next_emb = remainder_embeddings[idx]\n",
    "                sim_prev = cosine_similarity([prev_emb], [candidate_emb])[0][0]\n",
    "                sim_next = cosine_similarity([next_emb], [candidate_emb])[0][0]\n",
    "                \n",
    "                if self.debug:\n",
    "                    print(f\"[DEBUG] Adjust Boundaries (Batch): Chunk {i} için aday text: '{candidate_texts[idx]}'\")\n",
    "                    print(f\"[DEBUG] Önceki chunk ile similarity: {sim_prev:.4f}\")\n",
    "                    print(f\"[DEBUG] Kalan kısım (remainder) ile similarity: {sim_next:.4f}\")\n",
    "                \n",
    "                if sim_prev > sim_next:\n",
    "                    # Transfer işlemini gerçekleştir.\n",
    "                    next_sentences = self.split_text_into_sentences(adjusted_chunks[i+1])\n",
    "                    candidate_text = \" \".join(next_sentences[:self.transfer_sentence_count])\n",
    "                    adjusted_chunks[i] = adjusted_chunks[i].strip() + \" \" + candidate_text\n",
    "                    adjusted_chunks[i+1] = \" \".join(next_sentences[self.transfer_sentence_count:])\n",
    "                    if self.debug:\n",
    "                        print(f\"[DEBUG] Chunk {i+1}'den {self.transfer_sentence_count} cümle chunk {i}'e transfer edildi.\")\n",
    "        return adjusted_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_based_refinement(self, chunks):\n",
    "        if len(chunks) < 2:\n",
    "            return chunks\n",
    "\n",
    "        # Tüm chunk'ler için batch embedding işlemi\n",
    "        vectors_start = time.time()\n",
    "        vectors = self.create_embeddings(chunks)\n",
    "        vectors = np.array(vectors)\n",
    "        vectors_end = time.time()\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Tüm chunk'lar için batch embedding süresi: {vectors_end - vectors_start:.4f} saniye.\")\n",
    "\n",
    "        # SciPy linkage çağrısı için zaman ölçümü\n",
    "        linkage_start = time.time()\n",
    "        Z = linkage(vectors, method='average', metric='cosine')\n",
    "        linkage_end = time.time()\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] SciPy linkage toplam süresi: {linkage_end - linkage_start:.4f} saniye.\")\n",
    "\n",
    "        # SciPy fcluster çağrısı için zaman ölçümü\n",
    "        fcluster_start = time.time()\n",
    "        cluster_labels = fcluster(Z, t=self.cluster_threshold, criterion='distance')\n",
    "        fcluster_end = time.time()\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] SciPy fcluster toplam süresi: {fcluster_end - fcluster_start:.4f} saniye.\")\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Linkage matrix: {Z}\")\n",
    "            print(f\"[DEBUG] Cluster labels: {cluster_labels}\")\n",
    "\n",
    "        cluster_dict = {}\n",
    "        chunk_indices = {}  # Her cluster için chunk indexlerini saklayalım\n",
    "        chunk_contents = {}  # Her cluster için ilk 200 karakterlik içerikleri saklayalım\n",
    "\n",
    "        for idx, (label, chunk) in enumerate(zip(cluster_labels, chunks)):\n",
    "            cluster_dict.setdefault(label, []).append(chunk)\n",
    "            chunk_indices.setdefault(label, []).append(idx)\n",
    "            chunk_contents.setdefault(label, []).append(f\"[Chunk {idx}]: {chunk[:200]}...\")\n",
    "\n",
    "        refined_chunks = []\n",
    "        for label, cluster in cluster_dict.items():\n",
    "            merged_chunk = \" \".join(cluster)\n",
    "            chunk_index_list = chunk_indices[label]\n",
    "            chunk_content_list = chunk_contents[label]\n",
    "\n",
    "            if len(cluster) > 1:\n",
    "                if len(merged_chunk) > 10000:\n",
    "                    # Birleşen chunk toplam uzunluğu 10,000 karakteri aşıyorsa, birleşmeden orijinal chunk'ları ekle\n",
    "                    refined_chunks.extend(cluster)\n",
    "                    if self.debug:\n",
    "                        print(f\"[DEBUG] Cluster {label} birleşmedi, çünkü toplam uzunluğu {len(merged_chunk)} karakter!\")\n",
    "                        print(f\"[DEBUG] Bu cluster'daki chunk'lar ayrı kaldı: {chunk_index_list}\")\n",
    "                else:\n",
    "                    refined_chunks.append(merged_chunk)\n",
    "                    if self.debug:\n",
    "                        print(f\"[DEBUG] Cluster {label} birleşti! Birleşen chunk'lar: {chunk_index_list}\")\n",
    "                        print(f\"[DEBUG] Chunk içerikleri:\\n\" + \"\\n\".join(chunk_content_list))\n",
    "            else:\n",
    "                refined_chunks.append(merged_chunk)\n",
    "                if self.debug:\n",
    "                    print(f\"[DEBUG] Cluster {label} tek başına kaldı: {chunk_index_list}\")\n",
    "\n",
    "        return refined_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(self, texts):\n",
    "        all_chunks = []\n",
    "        for text in texts:\n",
    "            # Adım 1: Preprocess + Dynamic Threshold\n",
    "            step1_start = time.time()\n",
    "            segments = self.rule_based_segmentation(text)\n",
    "            initial_chunks = self.semantic_merging(segments)\n",
    "            step1_total = time.time() - step1_start\n",
    "            if self.debug:\n",
    "                print(f\"[DEBUG] Step 1 (Preprocess + Dynamic Threshold) toplam süresi: {step1_total:.4f} saniye.\")\n",
    "            \n",
    "            # Adım 2: Adjust Boundaries\n",
    "            step2_start = time.time()\n",
    "            adjusted_chunks = self.adjust_boundaries(initial_chunks)\n",
    "            step2_total = time.time() - step2_start\n",
    "            if self.debug:\n",
    "                print(f\"[DEBUG] Step 2 (Adjust Boundaries) toplam süresi: {step2_total:.4f} saniye.\")\n",
    "            \n",
    "            # Adım 3: Cluster (Topic-based Refinement)\n",
    "            step3_start = time.time()\n",
    "            refined_chunks = self.topic_based_refinement(adjusted_chunks)\n",
    "            step3_total = time.time() - step3_start\n",
    "            if self.debug:\n",
    "                print(f\"[DEBUG] Step 3 (Cluster) toplam süresi: {step3_total:.4f} saniye.\")\n",
    "            \n",
    "            all_chunks.extend(refined_chunks)\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=embedding_model_name)\n",
    "\n",
    "chunker = HScipyHybridSemanticChunker2(\n",
    "    embeddings, \n",
    "    similarity_threshold=None, \n",
    "    cluster_threshold=0.08, \n",
    "    window_size=6,  \n",
    "    transfer_sentence_count=2,\n",
    "    debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
