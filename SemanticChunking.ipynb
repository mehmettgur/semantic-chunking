{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "class SemanticChunking:\n",
    "    def __init__(self, embeddings, similarity_threshold, window_size, transfer_sentence_count):\n",
    "        self.embeddings = embeddings\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.window_size = window_size\n",
    "        self.transfer_sentence_count = transfer_sentence_count\n",
    "\n",
    "    def split_segment_by_word_boundary(self, segment, max_length):\n",
    "        words = segment.split()\n",
    "        parts = []\n",
    "        current_part = \"\"\n",
    "        for word in words:\n",
    "            if len(current_part) + len(word) + 1 > max_length:\n",
    "                parts.append(current_part.strip())\n",
    "                current_part = word\n",
    "            else:\n",
    "                current_part = f\"{current_part} {word}\" if current_part else word\n",
    "        if current_part.strip():\n",
    "            parts.append(current_part.strip())\n",
    "        return parts\n",
    "\n",
    "    def preprocess_segments(self, segments):\n",
    "        processed = []\n",
    "        for seg in segments:\n",
    "            seg = seg.strip()\n",
    "            if len(seg) < 3:\n",
    "                continue\n",
    "            if len(seg) > 10000:\n",
    "                processed.extend(self.split_segment_by_word_boundary(seg, 10000))\n",
    "            else:\n",
    "                processed.append(seg)\n",
    "        return processed\n",
    "\n",
    "    def split_text_into_sentences(self, text: str) -> list:\n",
    "        pattern = r'(?<=[a-zA-Z0-9])([.!])(?=\\s*[A-Z])|(?<=\\n)' \n",
    "        temp_parts = re.split(pattern, text)\n",
    "        temp_parts = [part if part is not None else \"\" for part in temp_parts]\n",
    "        reattached_sentences = []\n",
    "        i = 0\n",
    "        while i < len(temp_parts):\n",
    "            chunk = temp_parts[i]\n",
    "            if i + 1 < len(temp_parts) and re.match(r'^[.!]$', temp_parts[i+1]):\n",
    "                chunk += temp_parts[i+1]\n",
    "                i += 1\n",
    "            chunk = chunk.strip()\n",
    "            if chunk:\n",
    "                reattached_sentences.append(chunk)\n",
    "            i += 1\n",
    "\n",
    "        merged_sentences = []\n",
    "        buffer = \"\"\n",
    "        for sentence in reattached_sentences:\n",
    "            if len(buffer) + len(sentence) < 1000: # pdf uzunluğuna bağlı değiştirilebilir\n",
    "                buffer = f\"{buffer} {sentence}\" if buffer else sentence\n",
    "            else:\n",
    "                if buffer:\n",
    "                    merged_sentences.append(buffer)\n",
    "                buffer = sentence\n",
    "        if buffer:\n",
    "            merged_sentences.append(buffer)\n",
    "        return merged_sentences\n",
    "\n",
    "    def rule_based_segmentation(self, text):\n",
    "        segments = self.split_text_into_sentences(text)\n",
    "        segments = self.preprocess_segments(segments)\n",
    "        return segments\n",
    "\n",
    "    def create_embeddings(self, texts: list) -> list:\n",
    "        return self.embeddings.embed_documents(texts)\n",
    "\n",
    "    def calculate_dynamic_threshold_from_divergences(self, divergences):\n",
    "        \"\"\"\n",
    "        Verilen divergence (fark) değerlerine göre dinamik bir eşik (threshold) hesaplar.\n",
    "        - Eğer divergence değerleri yoksa varsayılan olarak 0.5 döner.\n",
    "        - Divergence'ların ortalaması ve standart sapması hesaplanır.\n",
    "        - Standart sapmanın küçük, orta veya büyük olmasına bağlı olarak farklı faktörler uygulanır.\n",
    "        - Sonuç, ortalama divergence ile standart sapmanın belirlenen faktörle çarpımının toplamıdır.\n",
    "        \"\"\"\n",
    "        if not divergences:\n",
    "            return 0.5\n",
    "        mean_div = sum(divergences) / len(divergences)\n",
    "        variance = sum((d - mean_div) ** 2 for d in divergences) / len(divergences)\n",
    "        std_div = math.sqrt(variance)\n",
    "        if std_div < 0.1:\n",
    "            factor = 1.5 # buradaki çarpımlar değiştirilebilir\n",
    "        elif std_div > 0.3:\n",
    "            factor = 1.0\n",
    "        else:\n",
    "            factor = 1.25\n",
    "        return mean_div + std_div * factor\n",
    "\n",
    "    def semantic_merging(self, segments):\n",
    "        n = len(segments)\n",
    "        if n < self.window_size:\n",
    "            return [\" \".join(segments)]\n",
    "        \n",
    "        embeddings = self.create_embeddings(segments)\n",
    "        split_points = set()\n",
    "        \n",
    "        for window_start in range(0, n - self.window_size + 1):\n",
    "            window_end = window_start + self.window_size\n",
    "            window_embeddings = embeddings[window_start:window_end]\n",
    "            window_divergences = []\n",
    "            for i in range(self.window_size - 1):\n",
    "                sim = cosine_similarity([window_embeddings[i]], [window_embeddings[i+1]])[0][0]\n",
    "                divergence = 1 - sim\n",
    "                window_divergences.append(divergence)\n",
    "            local_threshold = self.calculate_dynamic_threshold_from_divergences(window_divergences)\n",
    "            \n",
    "            for i, div in enumerate(window_divergences):\n",
    "                if div > local_threshold:\n",
    "                    global_index = window_start + i + 1\n",
    "                    split_points.add(global_index)\n",
    "        \n",
    "        split_points = sorted(list(split_points))\n",
    "        chunks = []\n",
    "        last_split = 0\n",
    "        for point in split_points:\n",
    "            chunk = \" \".join(segments[last_split:point])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            last_split = point\n",
    "        if last_split < n:\n",
    "            chunk = \" \".join(segments[last_split:])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def adjust_boundaries(self, chunks):\n",
    "        \"\"\"\n",
    "        Chunk'lar arasındaki sınırları ayarlamak için kullanılır.\n",
    "        - Her iki chunk arasındaki geçiş bölgesinde, belirli sayıda cümlenin transferi ile\n",
    "          daha uyumlu bir sınır elde edilmesi amaçlanır.\n",
    "        - Bir sonraki chunk'ın ilk 'transfer_sentence_count' cümlesi aday olarak alınır.\n",
    "        - Aday metnin, önceki chunk ve kalan kısmıyla olan benzerliği karşılaştırılarak,\n",
    "          eğer önceki chunk ile olan benzerlik daha yüksekse, aday cümleler önceki chunk'a eklenir.\n",
    "        \"\"\"\n",
    "        adjusted_chunks = chunks.copy()\n",
    "        candidate_texts = []\n",
    "        previous_texts = []\n",
    "        remainder_texts = []\n",
    "        indices = []\n",
    "        \n",
    "        for i in range(len(adjusted_chunks) - 1):\n",
    "            next_sentences = self.split_text_into_sentences(adjusted_chunks[i+1])\n",
    "            if not next_sentences or len(next_sentences) <= self.transfer_sentence_count:\n",
    "                continue\n",
    "            candidate_text = \" \".join(next_sentences[:self.transfer_sentence_count])\n",
    "            remainder = \" \".join(next_sentences[self.transfer_sentence_count:])\n",
    "            candidate_texts.append(candidate_text)\n",
    "            previous_texts.append(adjusted_chunks[i])\n",
    "            remainder_texts.append(remainder)\n",
    "            indices.append(i)\n",
    "        \n",
    "        if candidate_texts:\n",
    "            candidate_embeddings = self.create_embeddings(candidate_texts)\n",
    "            previous_embeddings = self.create_embeddings(previous_texts)\n",
    "            remainder_embeddings = self.create_embeddings(remainder_texts)\n",
    "        \n",
    "            for idx, i in enumerate(indices):\n",
    "                candidate_emb = candidate_embeddings[idx]\n",
    "                prev_emb = previous_embeddings[idx]\n",
    "                next_emb = remainder_embeddings[idx]\n",
    "                sim_prev = cosine_similarity([prev_emb], [candidate_emb])[0][0]\n",
    "                sim_next = cosine_similarity([next_emb], [candidate_emb])[0][0]\n",
    "                \n",
    "                if sim_prev > sim_next:\n",
    "                    next_sentences = self.split_text_into_sentences(adjusted_chunks[i+1])\n",
    "                    candidate_text = \" \".join(next_sentences[:self.transfer_sentence_count])\n",
    "                    adjusted_chunks[i] = adjusted_chunks[i].strip() + \" \" + candidate_text\n",
    "                    adjusted_chunks[i+1] = \" \".join(next_sentences[self.transfer_sentence_count:])\n",
    "        return adjusted_chunks\n",
    "\n",
    "    def create_documents(self, texts):\n",
    "        all_chunks = []\n",
    "        for text in texts:\n",
    "            segments = self.rule_based_segmentation(text)\n",
    "            initial_chunks = self.semantic_merging(segments)\n",
    "            adjusted_chunks = self.adjust_boundaries(initial_chunks)\n",
    "            final_chunks = []\n",
    "            for chunk in adjusted_chunks:\n",
    "                if len(chunk) > 10000:\n",
    "                    final_chunks.extend(self.split_segment_by_word_boundary(chunk, 10000))\n",
    "                else:\n",
    "                    final_chunks.append(chunk)\n",
    "            all_chunks.extend(final_chunks)\n",
    "        return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ve Qdrant işlemleri\n",
    "import json\n",
    "import uuid\n",
    "import openai\n",
    "import PyPDF2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "OPENAI_API_KEY = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "collection_name = \"hybrid-2-pdf-LLM-scipy_data_collection\"\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=embedding_model_name)\n",
    "\n",
    "chunker = SemanticChunking(\n",
    "    embeddings, \n",
    "    similarity_threshold=None, \n",
    "    window_size=6,  \n",
    "    transfer_sentence_count=2 # değiştirilebilir\n",
    ")\n",
    "\n",
    "pdf_path = \"Foundations of LLM.pdf\"\n",
    "documents = []\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text.append(page_text)\n",
    "    documents = [\"\\n\".join(full_text)]\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunker.create_documents([doc])\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "vector_size = 1536\n",
    "\n",
    "existing_collections = [c.name for c in qdrant_client.get_collections().collections]\n",
    "if collection_name in existing_collections:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"{collection_name} koleksiyonu silindi, yeniden oluşturuluyor...\")\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    ")\n",
    "print(f\"{collection_name} koleksiyonu başarıyla oluşturuldu.\")\n",
    "\n",
    "points = []\n",
    "for chunk in all_chunks:\n",
    "    vector = embeddings.embed_query(chunk)\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\"text\": chunk}\n",
    "    }\n",
    "    points.append(point)\n",
    "\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "print(f\"{len(points)} adet semantic chunk başarıyla Qdrant koleksiyonuna eklendi.\")\n",
    "\n",
    "def test_rag_direct_qdrant(qdrant_client, collection_name, query):\n",
    "    print(f\"\\nSorgu: {query}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [result.payload[\"text\"] for result in search_results]\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        print(\"Qdrant'tan eşleşen belge bulunamadı.\")\n",
    "        return\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    print(\"\\nQdrant'tan Kullanılan Kaynaklar:\")\n",
    "    for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "        print(f\"{idx}. (Uzunluk: {len(doc)}) {doc[:100]}...\")\n",
    "        \n",
    "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer: \"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that provides precise answers based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nInference zamanı: {round(end_time - start_time, 3)} saniye\")\n",
    "    print(f\"\\nModelin cevabı:\\n{response.choices[0].message.content}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
