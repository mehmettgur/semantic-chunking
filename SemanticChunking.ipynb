{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "class SemanticChunking:\n",
    "    def __init__(self, embeddings, similarity_threshold, window_size, transfer_sentence_count):\n",
    "        self.embeddings = embeddings\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.window_size = window_size\n",
    "        self.transfer_sentence_count = transfer_sentence_count\n",
    "\n",
    "    def split_segment_by_word_boundary(self, segment, max_length):\n",
    "        words = segment.split()\n",
    "        parts = []\n",
    "        current_part = \"\"\n",
    "        for word in words:\n",
    "            if len(current_part) + len(word) + 1 > max_length:\n",
    "                parts.append(current_part.strip())\n",
    "                current_part = word\n",
    "            else:\n",
    "                current_part = f\"{current_part} {word}\" if current_part else word\n",
    "        if current_part.strip():\n",
    "            parts.append(current_part.strip())\n",
    "        return parts\n",
    "\n",
    "    def preprocess_segments(self, segments):\n",
    "        processed = []\n",
    "        for seg in segments:\n",
    "            seg = seg.strip()\n",
    "            if len(seg) < 3:\n",
    "                continue\n",
    "            if len(seg) > 10000:\n",
    "                processed.extend(self.split_segment_by_word_boundary(seg, 10000))\n",
    "            else:\n",
    "                processed.append(seg)\n",
    "        return processed\n",
    "\n",
    "    def split_text_into_sentences(self, text: str) -> list:\n",
    "        pattern = r'(?<=[a-zA-Z0-9])([.!])(?=\\s*[A-Z])|(?<=\\n)' \n",
    "        temp_parts = re.split(pattern, text)\n",
    "        temp_parts = [part if part is not None else \"\" for part in temp_parts]\n",
    "        reattached_sentences = []\n",
    "        i = 0\n",
    "        while i < len(temp_parts):\n",
    "            chunk = temp_parts[i]\n",
    "            if i + 1 < len(temp_parts) and re.match(r'^[.!]$', temp_parts[i+1]):\n",
    "                chunk += temp_parts[i+1]\n",
    "                i += 1\n",
    "            chunk = chunk.strip()\n",
    "            if chunk:\n",
    "                reattached_sentences.append(chunk)\n",
    "            i += 1\n",
    "\n",
    "        merged_sentences = []\n",
    "        buffer = \"\"\n",
    "        for sentence in reattached_sentences:\n",
    "            if len(buffer) + len(sentence) < 1000: # pdf uzunluğuna bağlı değiştirilebilir\n",
    "                buffer = f\"{buffer} {sentence}\" if buffer else sentence\n",
    "            else:\n",
    "                if buffer:\n",
    "                    merged_sentences.append(buffer)\n",
    "                buffer = sentence\n",
    "        if buffer:\n",
    "            merged_sentences.append(buffer)\n",
    "        return merged_sentences\n",
    "\n",
    "    def rule_based_segmentation(self, text):\n",
    "        segments = self.split_text_into_sentences(text)\n",
    "        segments = self.preprocess_segments(segments)\n",
    "        return segments\n",
    "\n",
    "    def create_embeddings(self, texts: list) -> list:\n",
    "        return self.embeddings.embed_documents(texts)\n",
    "\n",
    "    def calculate_dynamic_threshold_from_divergences(self, divergences):\n",
    "        \"\"\"\n",
    "        Verilen divergence değerlerine göre her window için dinamik bir threshold hesaplar.\n",
    "        - Divergence'ların ortalaması ve standart sapması hesaplanır.\n",
    "        - Standart sapmanın küçük, orta veya büyük olmasına bağlı olarak farklı faktörler uygulanır.\n",
    "        \"\"\"\n",
    "        mean_div = sum(divergences) / len(divergences)\n",
    "        variance = sum((d - mean_div) ** 2 for d in divergences) / len(divergences)\n",
    "        std_div = math.sqrt(variance)\n",
    "        if std_div < 0.1:\n",
    "            factor = 1.5 # buradaki çarpımlar değiştirilebilir\n",
    "        elif std_div > 0.3:\n",
    "            factor = 1.0\n",
    "        else:\n",
    "            factor = 1.25\n",
    "        return mean_div + std_div * factor\n",
    "\n",
    "    def semantic_merging(self, segments):\n",
    "        n = len(segments)\n",
    "        if n < self.window_size:\n",
    "            return [\" \".join(segments)]\n",
    "        \n",
    "        embeddings = self.create_embeddings(segments)\n",
    "        split_points = set()\n",
    "        \n",
    "        for window_start in range(0, n - self.window_size + 1):\n",
    "            window_end = window_start + self.window_size\n",
    "            window_embeddings = embeddings[window_start:window_end]\n",
    "            window_divergences = []\n",
    "            for i in range(self.window_size - 1):\n",
    "                sim = cosine_similarity([window_embeddings[i]], [window_embeddings[i+1]])[0][0]\n",
    "                divergence = 1 - sim\n",
    "                window_divergences.append(divergence)\n",
    "            local_threshold = self.calculate_dynamic_threshold_from_divergences(window_divergences)\n",
    "            \n",
    "            for i, div in enumerate(window_divergences):\n",
    "                if div > local_threshold:\n",
    "                    global_index = window_start + i + 1\n",
    "                    split_points.add(global_index)\n",
    "        \n",
    "        split_points = sorted(list(split_points))\n",
    "        chunks = []\n",
    "        last_split = 0\n",
    "        for point in split_points:\n",
    "            chunk = \" \".join(segments[last_split:point])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            last_split = point\n",
    "        if last_split < n:\n",
    "            chunk = \" \".join(segments[last_split:])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def adjust_boundaries(self, chunks):\n",
    "        \"\"\"\n",
    "        Chunk'lar arasındaki sınırları ayarlamak için kullanılır.\n",
    "        - Her iki chunk arasındaki geçiş bölgesinde, belirli sayıda cümlenin transferi ile\n",
    "          daha uyumlu bir sınır elde edilmesi amaçlanır.\n",
    "        - Bir sonraki chunk'ın ilk 'transfer_sentence_count' cümlesi aday olarak alınır.\n",
    "        - Aday metnin, önceki chunk ve kalan kısmıyla olan benzerliği karşılaştırılarak,\n",
    "          eğer önceki chunk ile olan benzerlik daha yüksekse, aday cümleler önceki chunk'a eklenir.\n",
    "        \"\"\"\n",
    "        adjusted_chunks = chunks.copy()\n",
    "        candidate_texts = []\n",
    "        previous_texts = []\n",
    "        remainder_texts = []\n",
    "        indices = []\n",
    "        \n",
    "        for i in range(len(adjusted_chunks) - 1):\n",
    "            next_sentences = self.split_text_into_sentences(adjusted_chunks[i+1])\n",
    "            if not next_sentences or len(next_sentences) <= self.transfer_sentence_count:\n",
    "                continue\n",
    "            candidate_text = \" \".join(next_sentences[:self.transfer_sentence_count])\n",
    "            remainder = \" \".join(next_sentences[self.transfer_sentence_count:])\n",
    "            candidate_texts.append(candidate_text)\n",
    "            previous_texts.append(adjusted_chunks[i])\n",
    "            remainder_texts.append(remainder)\n",
    "            indices.append(i)\n",
    "        \n",
    "        if candidate_texts:\n",
    "            candidate_embeddings = self.create_embeddings(candidate_texts)\n",
    "            previous_embeddings = self.create_embeddings(previous_texts)\n",
    "            remainder_embeddings = self.create_embeddings(remainder_texts)\n",
    "        \n",
    "            for idx, i in enumerate(indices):\n",
    "                candidate_emb = candidate_embeddings[idx]\n",
    "                prev_emb = previous_embeddings[idx]\n",
    "                next_emb = remainder_embeddings[idx]\n",
    "                sim_prev = cosine_similarity([prev_emb], [candidate_emb])[0][0]\n",
    "                sim_next = cosine_similarity([next_emb], [candidate_emb])[0][0]\n",
    "                \n",
    "                if sim_prev > sim_next:\n",
    "                    next_sentences = self.split_text_into_sentences(adjusted_chunks[i+1])\n",
    "                    candidate_text = \" \".join(next_sentences[:self.transfer_sentence_count])\n",
    "                    adjusted_chunks[i] = adjusted_chunks[i].strip() + \" \" + candidate_text\n",
    "                    adjusted_chunks[i+1] = \" \".join(next_sentences[self.transfer_sentence_count:])\n",
    "        return adjusted_chunks\n",
    "\n",
    "    def create_documents(self, texts):\n",
    "        all_chunks = []\n",
    "        for text in texts:\n",
    "            segments = self.rule_based_segmentation(text)\n",
    "            initial_chunks = self.semantic_merging(segments)\n",
    "            adjusted_chunks = self.adjust_boundaries(initial_chunks)\n",
    "            final_chunks = []\n",
    "            for chunk in adjusted_chunks:\n",
    "                if len(chunk) > 10000:\n",
    "                    final_chunks.extend(self.split_segment_by_word_boundary(chunk, 10000))\n",
    "                else:\n",
    "                    final_chunks.append(chunk)\n",
    "            all_chunks.extend(final_chunks)\n",
    "        return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid-2-pdf-LLM-scipy_data_collection koleksiyonu silindi, yeniden oluşturuluyor...\n",
      "hybrid-2-pdf-LLM-scipy_data_collection koleksiyonu başarıyla oluşturuldu.\n",
      "136 adet semantic chunk başarıyla Qdrant koleksiyonuna eklendi.\n"
     ]
    }
   ],
   "source": [
    "# RAG ve Qdrant işlemleri\n",
    "import json\n",
    "import uuid\n",
    "import openai\n",
    "import PyPDF2\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "OPENAI_API_KEY = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "collection_name = \"hybrid-2-pdf-LLM-scipy_data_collection\"\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=embedding_model_name)\n",
    "\n",
    "chunker = SemanticChunking(\n",
    "    embeddings, \n",
    "    similarity_threshold=None, \n",
    "    window_size=6,  \n",
    "    transfer_sentence_count=2 # değiştirilebilir\n",
    ")\n",
    "\n",
    "pdf_path = \"Foundations of LLM.pdf\"\n",
    "documents = []\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text.append(page_text)\n",
    "    documents = [\"\\n\".join(full_text)]\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunker.create_documents([doc])\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "vector_size = 1536\n",
    "\n",
    "existing_collections = [c.name for c in qdrant_client.get_collections().collections]\n",
    "if collection_name in existing_collections:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"{collection_name} koleksiyonu silindi, yeniden oluşturuluyor.\")\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    ")\n",
    "print(f\"{collection_name} koleksiyonu başarıyla oluşturuldu.\")\n",
    "\n",
    "points = []\n",
    "for chunk in all_chunks:\n",
    "    vector = embeddings.embed_query(chunk)\n",
    "    point = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"vector\": vector,\n",
    "        \"payload\": {\"text\": chunk}\n",
    "    }\n",
    "    points.append(point)\n",
    "\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "print(f\"{len(points)} adet semantic chunk başarıyla Qdrant koleksiyonuna eklendi.\")\n",
    "\n",
    "def test_rag_direct_qdrant(qdrant_client, collection_name, query):\n",
    "    print(f\"\\nSorgu: {query}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [result.payload[\"text\"] for result in search_results]\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        print(\"Qdrant'tan eşleşen belge bulunamadı.\")\n",
    "        return\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    print(\"\\nQdrant'tan Kullanılan Kaynaklar:\")\n",
    "    for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "        print(f\"{idx}. (Uzunluk: {len(doc)}) {doc[:100]}...\")\n",
    "        \n",
    "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer: \"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that provides precise answers based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nInference zamanı: {round(end_time - start_time, 3)} saniye\")\n",
    "    print(f\"\\nModelin cevabı:\\n{response.choices[0].message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 2574) neural network with parameters θ, and odenotes the output of the neural network. Different problem s...\n",
      "2. (Uzunluk: 2710) forgetting problem in continual training, where a neural network forge ts previously learned in- for...\n",
      "3. (Uzunluk: 5110) The training objective can be deﬁned as (ˆθ,ˆω) = arg max θ,ωLoss(Model θ,ω(xnoise),x) (1.16) Here t...\n",
      "4. (Uzunluk: 4839) The ﬁne-tuned model is then employed to classify new sequences for this task. An advantage of superv...\n",
      "5. (Uzunluk: 965) 1Here we assume that tokens are basic units of text that are sep arated through tokenization. Someti...\n",
      "\n",
      "Inference zamanı: 2.391 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Pre-training refers to the process of optimizing a neural network on a task or set of tasks before it is further trained or fine-tuned for specific downstream tasks. The goal of pre-training is to create a model that can generalize across various tasks, reducing the reliance on task-specific labeled data. This process can involve unsupervised, supervised, or self-supervised learning approaches, where the model learns from either unlabeled data, labeled data, or generates its own supervision signals from the data, respectively. Pre-training helps in discovering better local minima and stabilizing the training process for subsequent supervised learning phases.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: Which types of models are widely used in NLP pre-training?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 965) 1Here we assume that tokens are basic units of text that are sep arated through tokenization. Someti...\n",
      "2. (Uzunluk: 3190) Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on...\n",
      "3. (Uzunluk: 5918) the discussion of these topics to the following chapters. CHAPTER 2 Generative Models One of the mos...\n",
      "4. (Uzunluk: 3861) The use of these ex- amples does not distinguish between models, but we mark the m odel architecture...\n",
      "5. (Uzunluk: 8279) in work using shared vocabularies, specifying the language to which a token belongs is not necessary...\n",
      "\n",
      "Inference zamanı: 2.428 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The types of models widely used in NLP pre-training include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)** - A Transformer encoder trained using masked language modeling and next sentence prediction tasks.\n",
      "2. **GPT (Generative Pre-trained Transformer)** - A generative model that focuses on predicting the next token in a sequence.\n",
      "3. **Transformer-based models** - These models utilize the Transformer architecture and can include various adaptations for different tasks.\n",
      "4. **LSTM-based models** - Long Short-Term Memory models that are used for sequence representation learning.\n",
      "5. **Word2Vec** - A method for learning word embeddings through word prediction tasks.\n",
      "6. **TinyBERT** - A distilled version of BERT aimed at improving efficiency while maintaining performance.\n",
      "\n",
      "These models leverage large-scale data and various pre-training tasks to learn representations that can be fine-tuned for specific downstream NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Which types of models are widely used in NLP pre-training?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: How do we implement permuted language modelling?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 5918) the discussion of these topics to the following chapters. CHAPTER 2 Generative Models One of the mos...\n",
      "2. (Uzunluk: 8453) The approach described above provides a new framework of uni versal language understanding and gener...\n",
      "3. (Uzunluk: 7725) sense to predict any of the tokens in this sequence. 1.2.2.1 Masked Language Modeling One of the mos...\n",
      "4. (Uzunluk: 5839) architecture to adapt LLMs to large-scale training. In Sect ion2.2we will present more discussions o...\n",
      "5. (Uzunluk: 5843) ↓ Is Next or Not? 1.2 Self-supervised Pre-training Tasks 13 x0 x0x1 x1x2 x2x3 x3x4 x4Pr(x0) = 1 Pr(x...\n",
      "\n",
      "Inference zamanı: 3.813 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "Permuted language modeling involves making sequential predictions of tokens in a non-linear order, rather than following the natural left-to-right or right-to-left order of the text. Here’s how to implement permuted language modeling:\n",
      "\n",
      "1. **Token Sequence Preparation**: Start with a sequence of tokens, for example, \\( x_0, x_1, x_2, x_3, x_4 \\).\n",
      "\n",
      "2. **Determine Prediction Order**: Decide on a new order for predicting the tokens. For instance, instead of predicting in the order \\( x_0 \\to x_1 \\to x_2 \\to x_3 \\to x_4 \\), you might choose \\( x_0 \\to x_4 \\to x_2 \\to x_1 \\to x_3 \\).\n",
      "\n",
      "3. **Modeling the Sequence**: The model should be trained to predict each token based on its context from the original sequence. For example, when predicting \\( x_1 \\), the model would consider the embeddings of \\( x_0, x_4, \\) and \\( x_2 \\).\n",
      "\n",
      "4. **Training the Model**: During training, use a standard language modeling approach where the probability of the sequence is modeled as:\n",
      "   \\[\n",
      "   Pr(x) = Pr(x_0) \\cdot Pr(x_4 | e_0) \\cdot Pr(x_2 | e_0, e_4) \\cdot Pr\n"
     ]
    }
   ],
   "source": [
    "query3 = \"How do we implement permuted language modelling?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorgu: What is the large-scale pre-training of the document?\n",
      "\n",
      "Qdrant'tan Kullanılan Kaynaklar:\n",
      "1. (Uzunluk: 2710) forgetting problem in continual training, where a neural network forge ts previously learned in- for...\n",
      "2. (Uzunluk: 965) 1Here we assume that tokens are basic units of text that are sep arated through tokenization. Someti...\n",
      "3. (Uzunluk: 4869) scaling laws for LLMs, which help us understand their traini ng efﬁciency and effectiveness. 2.2.1 D...\n",
      "4. (Uzunluk: 9996) D dataset used for training or ﬁne-tuning a model ∂L ∂θgradient of the loss function Lwith respect t...\n",
      "5. (Uzunluk: 9182) example, in He et al. [2021 ]’s work, a 1.5 billion-parameter BERT-like model is built b y increasin...\n",
      "\n",
      "Inference zamanı: 2.315 saniye\n",
      "\n",
      "Modelin cevabı:\n",
      "The large-scale pre-training discussed in the document refers to the process of training neural sequence models, particularly in natural language processing (NLP), on vast amounts of unlabeled data using self-supervised learning techniques. This approach allows models to learn general language understanding and generation by predicting masked words in large datasets, effectively creating foundation models that can be fine-tuned or adapted to specific downstream tasks. Examples of such models include BERT and GPT, which have demonstrated remarkable capabilities in various NLP tasks due to their pre-training on extensive corpora, such as web pages, books, and other text sources. The document highlights the significance of data quality, diversity, and the challenges associated with training large models, including issues related to bias and privacy.\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is the large-scale pre-training of the document?\"\n",
    "test_rag_direct_qdrant(qdrant_client, collection_name, query4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
